{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Love Behavior with Human Following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Create display widgets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "\n",
    "# Create widgets for displaying images\n",
    "display_color = widgets.Image(format='jpeg', width='60%')\n",
    "display_depth = widgets.Image(format='jpeg', width='60%')\n",
    "layout = widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth], layout=layout)\n",
    "display(sidebyside)\n",
    "\n",
    "# Convert numpy array to jpeg coded data for displaying\n",
    "def bgr8_to_jpeg(value):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Load YOLO model for human detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"yolo11l_half.engine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Initialize camera and implement love behavior with human following**\n",
    "\n",
    "The robot will:\n",
    "- Follow detected human by adjusting orientation (turn left/right)\n",
    "- Approach to maintain optimal distance (450-600mm)\n",
    "- Stop when human is at comfortable distance\n",
    "- Only follow humans, not all objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "import numpy as np\n",
    "import pyzed.sl as sl\n",
    "import threading\n",
    "import motors\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "# Initialize the Robot class\n",
    "robot = motors.MotorsYukon(mecanum=False)\n",
    "\n",
    "class Camera(SingletonConfigurable):\n",
    "    color_value = traitlets.Any()  # Monitor color_value variable\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "\n",
    "        self.zed = sl.Camera()\n",
    "        init_params = sl.InitParameters()\n",
    "        init_params.camera_resolution = sl.RESOLUTION.VGA\n",
    "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "        init_params.coordinate_units = sl.UNIT.MILLIMETER\n",
    "\n",
    "        status = self.zed.open(init_params)\n",
    "        if status != sl.ERROR_CODE.SUCCESS:\n",
    "            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "        self.thread_runnning_flag = False\n",
    "\n",
    "        camera_info = self.zed.get_camera_information()\n",
    "        self.width = camera_info.camera_configuration.resolution.width\n",
    "        self.height = camera_info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(self.width, self.height, sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(self.width, self.height, sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "        self.point_cloud = sl.Mat(self.width, self.height, sl.MAT_TYPE.F32_C4, sl.MEM.CPU)\n",
    "\n",
    "    def _capture_frames(self):\n",
    "        while(self.thread_runnning_flag == True):\n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "                \n",
    "                # Retrieve Left image\n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "                # Retrieve depth map\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "    \n",
    "                self.color_value_BGRA = self.image.get_data()\n",
    "                self.color_value = cv2.cvtColor(self.color_value_BGRA, cv2.COLOR_BGRA2BGR)\n",
    "                self.depth_image = np.asanyarray(self.depth.get_data())\n",
    "                \n",
    "    def start(self):\n",
    "        if self.thread_runnning_flag == False:\n",
    "            self.thread_runnning_flag = True\n",
    "            self.thread = threading.Thread(target=self._capture_frames)\n",
    "            self.thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False\n",
    "            self.thread.join()\n",
    "            robot.stop()\n",
    "\n",
    "camera = Camera()\n",
    "camera.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Implement love behavior with human following callback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def love_callback(change):\n",
    "    frame = change['new']\n",
    "    \n",
    "    # Run YOLO inference to detect humans\n",
    "    results = model(frame, verbose=False)\n",
    "    \n",
    "    # Prepare depth colormap for display\n",
    "    depth_colormap = cv2.applyColorMap(\n",
    "        cv2.convertScaleAbs(camera.depth_image, alpha=0.03), \n",
    "        cv2.COLORMAP_JET)\n",
    "    \n",
    "    conf_threshold = 0.5\n",
    "    human_detected = False\n",
    "    closest_distance = float('inf')\n",
    "    closest_bbox = None\n",
    "    \n",
    "    # Check for human detections (class 0)\n",
    "    for result in results:\n",
    "        for i in range(len(result.boxes.cls)):\n",
    "            if result.boxes.cls[i] == 0:  # Human subject\n",
    "                if result.boxes.conf[i] > conf_threshold:\n",
    "                    bbox = result.boxes.xyxy[i]\n",
    "                    \n",
    "                    # Draw detection box\n",
    "                    cv2.rectangle(frame, \n",
    "                                (int(bbox[0]), int(bbox[1])), \n",
    "                                (int(bbox[2]), int(bbox[3])), \n",
    "                                (255, 105, 180), 2)  # Pink for love\n",
    "                    \n",
    "                    # Calculate center of bounding box\n",
    "                    center_x = int((bbox[0] + bbox[2]) / 2)\n",
    "                    center_y = int((bbox[1] + bbox[3]) / 2)\n",
    "                    \n",
    "                    # Get depth at human center (handle NaN)\n",
    "                    if (0 <= center_y < camera.height and \n",
    "                        0 <= center_x < camera.width):\n",
    "                        distance = camera.depth_image[center_y, center_x]\n",
    "                        \n",
    "                        if not np.isnan(distance) and distance > 0:\n",
    "                            if distance < closest_distance:\n",
    "                                human_detected = True\n",
    "                                closest_distance = distance\n",
    "                                closest_bbox = bbox\n",
    "    \n",
    "    # LOVE BEHAVIOR - follow human with differential drive\n",
    "    if human_detected and closest_distance != float('inf'):\n",
    "        # Calculate human position relative to frame center\n",
    "        bbox_center_x = (closest_bbox[0] + closest_bbox[2]) / 2\n",
    "        frame_center_x = camera.width / 2\n",
    "        \n",
    "        # Calculate angle offset (normalized to -1 to 1)\n",
    "        offset_x = bbox_center_x - frame_center_x\n",
    "        max_offset = camera.width / 2\n",
    "        normalized_angle = offset_x / max_offset  # Range: -1 (left) to 1 (right)\n",
    "        \n",
    "        # Convert to radians for differential drive (max Â±Ï€/4 for smooth turning)\n",
    "        angle_rad = normalized_angle * (np.pi / 4)\n",
    "        \n",
    "        # Calculate differential drive speeds using cosine\n",
    "        turning_factor = np.cos(angle_rad)\n",
    "        \n",
    "        # LOVE BEHAVIOR - approach and follow with differential drive\n",
    "        if closest_distance < 450:\n",
    "            # Too close - slight retreat\n",
    "            robot.backward(0.2)\n",
    "            cv2.putText(depth_colormap, f'Too close {closest_distance:.0f}mm', \n",
    "                       (200, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "            print(f'Love: Too close {closest_distance:.0f}mm - backing up', end='\\r')\n",
    "            \n",
    "        elif closest_distance >= 450 and closest_distance <= 600:\n",
    "            # Optimal distance - stop and enjoy proximity\n",
    "            robot.stop()\n",
    "            cv2.putText(depth_colormap, f'Perfect {closest_distance:.0f}mm <3', \n",
    "                       (200, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2, cv2.LINE_AA)\n",
    "            print(f'Love: Perfect distance {closest_distance:.0f}mm', end='\\r')\n",
    "            \n",
    "        elif closest_distance > 600 and closest_distance <= 1200:\n",
    "            # Medium distance - approach with differential steering\n",
    "            base_speed = 0.3\n",
    "            \n",
    "            if angle_rad > 0.1:  # Human on right\n",
    "                # Turn right: slow down right wheels\n",
    "                left_speed = base_speed\n",
    "                right_speed = base_speed * turning_factor\n",
    "                robot.frontLeft(speed=left_speed)\n",
    "                robot.backLeft(speed=left_speed)\n",
    "                robot.frontRight(speed=right_speed)\n",
    "                robot.backRight(speed=right_speed)\n",
    "                cv2.putText(depth_colormap, f'Following right {closest_distance:.0f}mm', \n",
    "                           (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 180, 0), 2, cv2.LINE_AA)\n",
    "                print(f'Love: Following right {closest_distance:.0f}mm (L:{left_speed:.2f} R:{right_speed:.2f})', end='\\r')\n",
    "                \n",
    "            elif angle_rad < -0.1:  # Human on left\n",
    "                # Turn left: slow down left wheels\n",
    "                left_speed = base_speed * turning_factor\n",
    "                right_speed = base_speed\n",
    "                robot.frontLeft(speed=left_speed)\n",
    "                robot.backLeft(speed=left_speed)\n",
    "                robot.frontRight(speed=right_speed)\n",
    "                robot.backRight(speed=right_speed)\n",
    "                cv2.putText(depth_colormap, f'Following left {closest_distance:.0f}mm', \n",
    "                           (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 180, 0), 2, cv2.LINE_AA)\n",
    "                print(f'Love: Following left {closest_distance:.0f}mm (L:{left_speed:.2f} R:{right_speed:.2f})', end='\\r')\n",
    "                \n",
    "            else:  # Human centered\n",
    "                # Move straight forward\n",
    "                robot.forward(base_speed)\n",
    "                cv2.putText(depth_colormap, f'Approaching {closest_distance:.0f}mm', \n",
    "                           (200, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 180, 0), 2, cv2.LINE_AA)\n",
    "                print(f'Love: Approaching {closest_distance:.0f}mm (straight)', end='\\r')\n",
    "                \n",
    "        else:  # closest_distance > 1200\n",
    "            # Far away - approach faster with differential steering\n",
    "            base_speed = 0.5\n",
    "            \n",
    "            if angle_rad > 0.1:  # Human on right\n",
    "                # Turn right: slow down right wheels\n",
    "                left_speed = base_speed\n",
    "                right_speed = base_speed * turning_factor\n",
    "                robot.frontLeft(speed=left_speed)\n",
    "                robot.backLeft(speed=left_speed)\n",
    "                robot.frontRight(speed=right_speed)\n",
    "                robot.backRight(speed=right_speed)\n",
    "                cv2.putText(depth_colormap, f'Searching right {closest_distance:.0f}mm', \n",
    "                           (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2, cv2.LINE_AA)\n",
    "                print(f'Love: Searching right {closest_distance:.0f}mm (L:{left_speed:.2f} R:{right_speed:.2f})', end='\\r')\n",
    "                \n",
    "            elif angle_rad < -0.1:  # Human on left\n",
    "                # Turn left: slow down left wheels\n",
    "                left_speed = base_speed * turning_factor\n",
    "                right_speed = base_speed\n",
    "                robot.frontLeft(speed=left_speed)\n",
    "                robot.backLeft(speed=left_speed)\n",
    "                robot.frontRight(speed=right_speed)\n",
    "                robot.backRight(speed=right_speed)\n",
    "                cv2.putText(depth_colormap, f'Searching left {closest_distance:.0f}mm', \n",
    "                           (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2, cv2.LINE_AA)\n",
    "                print(f'Love: Searching left {closest_distance:.0f}mm (L:{left_speed:.2f} R:{right_speed:.2f})', end='\\r')\n",
    "                \n",
    "            else:  # Human centered\n",
    "                # Move straight forward quickly\n",
    "                robot.forward(base_speed)\n",
    "                cv2.putText(depth_colormap, f'Coming to you {closest_distance:.0f}mm', \n",
    "                           (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2, cv2.LINE_AA)\n",
    "                print(f'Love: Fast approach {closest_distance:.0f}mm (straight)', end='\\r')\n",
    "                \n",
    "    else:\n",
    "        # No human detected - stop and wait\n",
    "        robot.stop()\n",
    "        cv2.putText(depth_colormap, 'Waiting for human...', \n",
    "                   (200, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 2, cv2.LINE_AA)\n",
    "        print('Love: No human detected - waiting', end='\\r')\n",
    "    \n",
    "    # Display images\n",
    "    scale = 0.1\n",
    "    resized_color = cv2.resize(frame, None, fx=scale, fy=scale, \n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "    resized_depth = cv2.resize(depth_colormap, None, fx=scale, fy=scale, \n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "    display_color.value = bgr8_to_jpeg(resized_color)\n",
    "    display_depth.value = bgr8_to_jpeg(resized_depth)\n",
    "\n",
    "# Attach callback to camera\n",
    "camera.observe(love_callback, names=['color_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To stop the camera and robot:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Behavior Description:**\n",
    "\n",
    "The robot exhibits love behavior by following detected humans:\n",
    "\n",
    "1. **Distance Measurement**: Calculates distance to closest detected human using depth image\n",
    "2. **Intelligent Tracking**: \n",
    "   - Calculates human position relative to frame center\n",
    "   - Only turns when human is >15% off-center (avoids continuous adjustment)\n",
    "   - Turns left/right to recenter human in view\n",
    "3. **Distance-Based Approach**:\n",
    "   - **< 450mm**: Slight backward retreat (0.2 speed)\n",
    "   - **450-600mm**: Stop - optimal \"loving\" distance\n",
    "   - **600-1200mm**: Slow approach (0.3) or turn to track (0.3)\n",
    "   - **> 1200mm**: Fast approach (0.5) or turn to track (0.4)\n",
    "4. **Smart Following**: \n",
    "   - When centered: moves forward toward human\n",
    "   - When off-center: rotates to recenter before/while approaching\n",
    "   - When lost: stops and waits (no continuous searching)\n",
    "\n",
    "The robot follows humans by maintaining visual tracking and optimal proximity, turning only when necessary to keep the human centered in its field of view."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
