{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fear Behavior with Human Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Create display widgets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "\n",
    "# Create widgets for displaying images\n",
    "display_color = widgets.Image(format='jpeg', width='30%')\n",
    "display_depth = widgets.Image(format='jpeg', width='30%')\n",
    "layout = widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth], layout=layout)\n",
    "display(sidebyside)\n",
    "\n",
    "# Convert numpy array to jpeg coded data for displaying\n",
    "def bgr8_to_jpeg(value):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Load YOLO model for human detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"yolo11l_half.engine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Initialize camera and implement fear behavior with human detection**\n",
    "\n",
    "The robot will:\n",
    "- Detect humans using YOLO\n",
    "- Calculate distance to detected humans using depth image\n",
    "- Retreat at different speeds based on distance\n",
    "- Rotate away if human is too close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "import numpy as np\n",
    "import pyzed.sl as sl\n",
    "import threading\n",
    "import motors\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "# Initialize the Robot class\n",
    "robot = motors.MotorsYukon(mecanum=False)\n",
    "\n",
    "class Camera(SingletonConfigurable):\n",
    "    color_value = traitlets.Any()  # Monitor color_value variable\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "\n",
    "        self.zed = sl.Camera()\n",
    "        init_params = sl.InitParameters()\n",
    "        init_params.camera_resolution = sl.RESOLUTION.VGA\n",
    "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "        init_params.coordinate_units = sl.UNIT.MILLIMETER\n",
    "\n",
    "        status = self.zed.open(init_params)\n",
    "        if status != sl.ERROR_CODE.SUCCESS:\n",
    "            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "        self.thread_runnning_flag = False\n",
    "\n",
    "        camera_info = self.zed.get_camera_information()\n",
    "        self.width = camera_info.camera_configuration.resolution.width\n",
    "        self.height = camera_info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(self.width, self.height, sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(self.width, self.height, sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "        self.point_cloud = sl.Mat(self.width, self.height, sl.MAT_TYPE.F32_C4, sl.MEM.CPU)\n",
    "\n",
    "    def _capture_frames(self):\n",
    "        while(self.thread_runnning_flag == True):\n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "                \n",
    "                # Retrieve Left image\n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "                # Retrieve depth map\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "    \n",
    "                self.color_value_BGRA = self.image.get_data()\n",
    "                self.color_value = cv2.cvtColor(self.color_value_BGRA, cv2.COLOR_BGRA2BGR)\n",
    "                self.depth_image = np.asanyarray(self.depth.get_data())\n",
    "                \n",
    "    def start(self):\n",
    "        if self.thread_runnning_flag == False:\n",
    "            self.thread_runnning_flag = True\n",
    "            self.thread = threading.Thread(target=self._capture_frames)\n",
    "            self.thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False\n",
    "            self.thread.join()\n",
    "            robot.stop()\n",
    "\n",
    "camera = Camera()\n",
    "camera.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Implement fear behavior with human detection callback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fear_callback(change):\n",
    "    frame = change['new']\n",
    "    \n",
    "    # Run YOLO inference to detect humans\n",
    "    results = model(frame, verbose=False)\n",
    "    \n",
    "    # Prepare depth colormap for display\n",
    "    depth_colormap = cv2.applyColorMap(\n",
    "        cv2.convertScaleAbs(camera.depth_image, alpha=0.03), \n",
    "        cv2.COLORMAP_JET)\n",
    "    \n",
    "    conf_threshold = 0.5\n",
    "    human_detected = False\n",
    "    min_distance = float('inf')\n",
    "    closest_bbox = None\n",
    "    \n",
    "    # Check for human detections (class 0)\n",
    "    for result in results:\n",
    "        for i in range(len(result.boxes.cls)):\n",
    "            if result.boxes.cls[i] == 0:  # Human subject\n",
    "                if result.boxes.conf[i] > conf_threshold:\n",
    "                    human_detected = True\n",
    "                    bbox = result.boxes.xyxy[i]\n",
    "                    \n",
    "                    # Draw detection box\n",
    "                    cv2.rectangle(frame, \n",
    "                                (int(bbox[0]), int(bbox[1])), \n",
    "                                (int(bbox[2]), int(bbox[3])), \n",
    "                                (255, 0, 0), 2)\n",
    "                    \n",
    "                    # Calculate center of bounding box\n",
    "                    center_x = int((bbox[0] + bbox[2]) / 2)\n",
    "                    center_y = int((bbox[1] + bbox[3]) / 2)\n",
    "                    \n",
    "                    # Get depth at human center (handle NaN)\n",
    "                    if (0 <= center_y < camera.height and \n",
    "                        0 <= center_x < camera.width):\n",
    "                        distance = camera.depth_image[center_y, center_x]\n",
    "                        \n",
    "                        if not np.isnan(distance) and distance > 0:\n",
    "                            if distance < min_distance:\n",
    "                                min_distance = distance\n",
    "                                closest_bbox = bbox\n",
    "    \n",
    "    # FEAR BEHAVIOR - graduated response based on closest human\n",
    "    if human_detected and min_distance != float('inf'):\n",
    "        # Determine if human is on left or right side\n",
    "        bbox_center_x = (closest_bbox[0] + closest_bbox[2]) / 2\n",
    "        frame_center_x = camera.width / 2\n",
    "        human_on_left = bbox_center_x < frame_center_x\n",
    "        \n",
    "        if min_distance < 500:\n",
    "            # Very close - retreat and rotate away\n",
    "            robot.backward(0.6)\n",
    "            if human_on_left:\n",
    "                robot.spinRight(0.3)\n",
    "            else:\n",
    "                robot.spinLeft(0.3)\n",
    "            cv2.putText(depth_colormap, f'DANGER {min_distance:.0f}mm - RETREAT!', \n",
    "                       (100, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            print(f'DANGER {min_distance:.0f}mm - Fast retreat & rotate', end='\\r')\n",
    "            \n",
    "        elif min_distance < 900:\n",
    "            # Medium distance - retreat\n",
    "            robot.backward(0.4)\n",
    "            cv2.putText(depth_colormap, f'CAUTION {min_distance:.0f}mm', \n",
    "                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 165, 255), 2, cv2.LINE_AA)\n",
    "            print(f'CAUTION {min_distance:.0f}mm - Medium retreat', end='\\r')\n",
    "            \n",
    "        elif min_distance < 1300:\n",
    "            # Far but still cautious - slow retreat\n",
    "            robot.backward(0.2)\n",
    "            cv2.putText(depth_colormap, f'ALERT {min_distance:.0f}mm', \n",
    "                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "            print(f'ALERT {min_distance:.0f}mm - Slow retreat', end='\\r')\n",
    "            \n",
    "        else:\n",
    "            # Safe distance - stop\n",
    "            robot.stop()\n",
    "            cv2.putText(depth_colormap, f'SAFE {min_distance:.0f}mm', \n",
    "                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            print(f'SAFE {min_distance:.0f}mm - Human at safe distance', end='\\r')\n",
    "    else:\n",
    "        # No human detected - stop\n",
    "        robot.stop()\n",
    "        cv2.putText(depth_colormap, 'SAFE - No human detected', \n",
    "                   (120, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        print('SAFE - No human threat detected', end='\\r')\n",
    "    \n",
    "    # Display images\n",
    "    scale = 0.1\n",
    "    resized_color = cv2.resize(frame, None, fx=scale, fy=scale, \n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "    resized_depth = cv2.resize(depth_colormap, None, fx=scale, fy=scale, \n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "    display_color.value = bgr8_to_jpeg(resized_color)\n",
    "    display_depth.value = bgr8_to_jpeg(resized_depth)\n",
    "\n",
    "# Attach callback to camera\n",
    "camera.observe(fear_callback, names=['color_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To stop the camera and robot:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Behavior Description:**\n",
    "\n",
    "The robot exhibits fear behavior specifically toward humans:\n",
    "\n",
    "1. **Distance Measurement**: Calculates distance to each detected human using depth image at bounding box center\n",
    "2. **Graduated Fear Response**:\n",
    "   - **< 500mm**: Fast backward retreat (0.6 speed) + rotate away from human\n",
    "   - **< 900mm**: Medium backward retreat (0.4 speed)\n",
    "   - **< 1300mm**: Slow backward retreat (0.2 speed)\n",
    "   - **>= 1300mm**: Stop - safe distance\n",
    "3. **Rotation Logic**: When very close (<500mm), robot rotates away from human's position (left or right)\n",
    "4. **No Human**: Robot stops when no humans detected\n",
    "\n",
    "The robot only fears humans, ignoring other objects in the environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
