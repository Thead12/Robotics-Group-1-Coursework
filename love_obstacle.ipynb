{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Love Behavior with Human Following",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Step 1: Create display widgets**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import ipywidgets.widgets as widgets\nfrom IPython.display import display\nimport cv2\n\n# Create widgets for displaying images\ndisplay_color = widgets.Image(format='jpeg', width='60%')\ndisplay_depth = widgets.Image(format='jpeg', width='60%')\nlayout = widgets.Layout(width='100%')\n\nsidebyside = widgets.HBox([display_color, display_depth], layout=layout)\ndisplay(sidebyside)\n\n# Convert numpy array to jpeg coded data for displaying\ndef bgr8_to_jpeg(value):\n    return bytes(cv2.imencode('.jpg', value)[1])",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9facf616fcfa4a1692e661bbc0499d9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(Image(value=b'', format='jpeg', width='60%'), Image(value=b'', format='jpeg', width='60%')), la…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "**Step 2: Load YOLO model for human detection**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from ultralytics import YOLO\n\n# Load the YOLO model\nmodel = YOLO(\"yolo11l_half.engine\")",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n"
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": "**Step 3: Initialize camera and implement love behavior with human following**\n\nThe robot will:\n- Follow detected human by adjusting orientation (turn left/right)\n- Approach to maintain optimal distance (450-600mm)\n- Stop when human is at comfortable distance\n- Only follow humans, not all objects",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import traitlets\nimport numpy as np\nimport pyzed.sl as sl\nimport threading\nimport motors\nfrom traitlets.config.configurable import SingletonConfigurable\n\n# Initialize the Robot class\nrobot = motors.MotorsYukon(mecanum=False)\n\nclass Camera(SingletonConfigurable):\n    color_value = traitlets.Any()  # Monitor color_value variable\n    \n    def __init__(self):\n        super(Camera, self).__init__()\n\n        self.zed = sl.Camera()\n        init_params = sl.InitParameters()\n        init_params.camera_resolution = sl.RESOLUTION.VGA\n        init_params.depth_mode = sl.DEPTH_MODE.ULTRA\n        init_params.coordinate_units = sl.UNIT.MILLIMETER\n\n        status = self.zed.open(init_params)\n        if status != sl.ERROR_CODE.SUCCESS:\n            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n            self.zed.close()\n            exit(1)\n\n        self.runtime = sl.RuntimeParameters()\n        self.thread_runnning_flag = False\n\n        camera_info = self.zed.get_camera_information()\n        self.width = camera_info.camera_configuration.resolution.width\n        self.height = camera_info.camera_configuration.resolution.height\n        self.image = sl.Mat(self.width, self.height, sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n        self.depth = sl.Mat(self.width, self.height, sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n        self.point_cloud = sl.Mat(self.width, self.height, sl.MAT_TYPE.F32_C4, sl.MEM.CPU)\n\n    def _capture_frames(self):\n        while(self.thread_runnning_flag == True):\n            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n                \n                # Retrieve Left image\n                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n                # Retrieve depth map\n                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n    \n                self.color_value_BGRA = self.image.get_data()\n                self.color_value = cv2.cvtColor(self.color_value_BGRA, cv2.COLOR_BGRA2BGR)\n                self.depth_image = np.asanyarray(self.depth.get_data())\n                \n    def start(self):\n        if self.thread_runnning_flag == False:\n            self.thread_runnning_flag = True\n            self.thread = threading.Thread(target=self._capture_frames)\n            self.thread.start()\n\n    def stop(self):\n        if self.thread_runnning_flag == True:\n            self.thread_runnning_flag = False\n            self.thread.join()\n            robot.stop()\n\ncamera = Camera()\ncamera.start()",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[2025-12-05 10:21:18 UTC][ZED][INFO] Logging level INFO\n,[2025-12-05 10:21:18 UTC][ZED][INFO] Logging level INFO\n,[2025-12-05 10:21:18 UTC][ZED][INFO] Logging level INFO\n,[2025-12-05 10:21:19 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n,[2025-12-05 10:21:20 UTC][ZED][INFO] [Init]  Camera successfully opened.\n,[2025-12-05 10:21:20 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n,[2025-12-05 10:21:20 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n,[2025-12-05 10:21:20 UTC][ZED][INFO] [Init]  Serial Number: S/N 36955685\n,Loading yolo11l_half.engine for TensorRT inference...\n,Love: No human detected - waitingp"
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": "**Step 4: Implement love behavior with human following callback**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 4: Enhanced person-following with obstacle avoidance and occlusion handling\n\ndef enhanced_follow_callback(change):\n    frame = change['new']\n    \n    # Run YOLO inference to detect humans\n    results = model(frame, verbose=False)\n    \n    # Prepare depth colormap for display\n    depth_colormap = cv2.applyColorMap(\n        cv2.convertScaleAbs(camera.depth_image, alpha=0.03), \n        cv2.COLORMAP_JET)\n    \n    conf_threshold = 0.5\n    human_detected = False\n    closest_distance = float('inf')\n    closest_bbox = None\n    all_humans = []  # Track all detected humans for occlusion handling\n    \n    # Detect all humans in frame\n    for result in results:\n        for i in range(len(result.boxes.cls)):\n            if result.boxes.cls[i] == 0:  # Human subject\n                if result.boxes.conf[i] > conf_threshold:\n                    bbox = result.boxes.xyxy[i]\n                    \n                    # Calculate center of bounding box\n                    center_x = int((bbox[0] + bbox[2]) / 2)\n                    center_y = int((bbox[1] + bbox[3]) / 2)\n                    \n                    # Get depth at human center\n                    if (0 <= center_y < camera.height and \n                        0 <= center_x < camera.width):\n                        distance = camera.depth_image[center_y, center_x]\n                        \n                        if not np.isnan(distance) and distance > 0:\n                            all_humans.append({\n                                'bbox': bbox,\n                                'distance': distance,\n                                'center_x': center_x,\n                                'center_y': center_y\n                            })\n                            \n                            if distance < closest_distance:\n                                human_detected = True\n                                closest_distance = distance\n                                closest_bbox = bbox\n    \n    # Draw all detected humans\n    for human in all_humans:\n        bbox = human['bbox']\n        # Pink for target (closest), yellow for others\n        color = (255, 105, 180) if bbox is closest_bbox else (0, 255, 255)\n        cv2.rectangle(frame, \n                     (int(bbox[0]), int(bbox[1])), \n                     (int(bbox[2]), int(bbox[3])), \n                     color, 2)\n    \n    # COLLISION AVOIDANCE using depth-based obstacle detection\n    # (Based on Tutorial 3 Part B collision avoidance)\n    depth_image_obstacles = camera.depth_image.copy()\n    depth_image_obstacles = np.nan_to_num(depth_image_obstacles, nan=0.0).astype(np.float32)\n    \n    # Define central collision detection area (from Tutorial 3)\n    depth_image_obstacles[:94, :] = 0\n    depth_image_obstacles[282:, :] = 0\n    depth_image_obstacles[:, :168] = 0\n    depth_image_obstacles[:, 504:] = 0\n    \n    # Filter depth values for obstacle detection\n    depth_image_obstacles[depth_image_obstacles < 100] = 0\n    depth_image_obstacles[depth_image_obstacles > 2000] = 0\n    \n    # Check for obstacles in path\n    obstacle_detected = False\n    obstacle_distance = float('inf')\n    if depth_image_obstacles.max() > 0:\n        obstacle_distance = depth_image_obstacles[depth_image_obstacles != 0].min()\n        if obstacle_distance < 400:  # Obstacle within collision threshold\n            obstacle_detected = True\n    \n    # ENHANCED FOLLOWING BEHAVIOR with obstacle avoidance\n    if human_detected and closest_distance != float('inf'):\n        # Calculate human position relative to frame center\n        bbox_center_x = (closest_bbox[0] + closest_bbox[2]) / 2\n        frame_center_x = camera.width / 2\n        \n        # Calculate horizontal offset from center\n        offset_x = bbox_center_x - frame_center_x\n        offset_threshold = camera.width * 0.15  # 15% of frame width\n        \n        # Determine if human needs recentering\n        needs_left_turn = offset_x < -offset_threshold\n        needs_right_turn = offset_x > offset_threshold\n        centered = not needs_left_turn and not needs_right_turn\n        \n        # PRIORITY 1: Emergency collision avoidance\n        if obstacle_detected and obstacle_distance < 300:\n            robot.backward(0.3)\n            cv2.putText(depth_colormap, f'COLLISION AVOID! {obstacle_distance:.0f}mm', \n                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2, cv2.LINE_AA)\n            cv2.putText(depth_colormap, 'RETREATING', \n                       (200, 208), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2, cv2.LINE_AA)\n            print(f'EMERGENCY: Obstacle at {obstacle_distance:.0f}mm - retreating', end='\\r')\n        \n        # PRIORITY 2: Cautious approach with obstacle awareness\n        elif obstacle_detected and obstacle_distance < 500:\n            # Obstacle close but not emergency - slow down and navigate around\n            if needs_left_turn:\n                robot.spinLeft(0.2)\n                cv2.putText(depth_colormap, f'CAUTION: Navigate left', \n                           (160, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2, cv2.LINE_AA)\n            elif needs_right_turn:\n                robot.spinRight(0.2)\n                cv2.putText(depth_colormap, f'CAUTION: Navigate right', \n                           (160, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2, cv2.LINE_AA)\n            else:\n                robot.forward(0.15)  # Very slow forward if centered\n                cv2.putText(depth_colormap, f'CAUTION: Slow approach', \n                           (160, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2, cv2.LINE_AA)\n            print(f'Caution: Obstacle {obstacle_distance:.0f}mm, Human {closest_distance:.0f}mm', end='\\r')\n        \n        # PRIORITY 3: Normal following behavior (no immediate obstacles)\n        elif closest_distance < 450:\n            # Too close to target person - retreat\n            robot.backward(0.2)\n            cv2.putText(depth_colormap, f'Too close {closest_distance:.0f}mm', \n                       (200, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2, cv2.LINE_AA)\n            print(f'Love: Too close {closest_distance:.0f}mm - backing up', end='\\r')\n            \n        elif closest_distance >= 450 and closest_distance <= 600:\n            # Optimal distance - maintain position\n            robot.stop()\n            cv2.putText(depth_colormap, f'Perfect {closest_distance:.0f}mm <3', \n                       (200, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2, cv2.LINE_AA)\n            print(f'Love: Perfect distance {closest_distance:.0f}mm', end='\\r')\n            \n        elif closest_distance > 600 and closest_distance <= 1200:\n            # Medium distance - approach while tracking\n            if centered:\n                robot.forward(0.3)\n                cv2.putText(depth_colormap, f'Following {closest_distance:.0f}mm', \n                           (200, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 180, 0), 2, cv2.LINE_AA)\n            elif needs_left_turn:\n                robot.spinLeft(0.3)\n                cv2.putText(depth_colormap, f'Tracking left {closest_distance:.0f}mm', \n                           (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 180, 0), 2, cv2.LINE_AA)\n            else:  # needs_right_turn\n                robot.spinRight(0.3)\n                cv2.putText(depth_colormap, f'Tracking right {closest_distance:.0f}mm', \n                           (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 180, 0), 2, cv2.LINE_AA)\n            print(f'Love: Following {closest_distance:.0f}mm', end='\\r')\n                \n        else:  # closest_distance > 1200\n            # Far away - approach faster\n            if centered:\n                robot.forward(0.5)\n                cv2.putText(depth_colormap, f'Approaching {closest_distance:.0f}mm', \n                           (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2, cv2.LINE_AA)\n            elif needs_left_turn:\n                robot.spinLeft(0.4)\n                cv2.putText(depth_colormap, f'Searching left {closest_distance:.0f}mm', \n                           (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2, cv2.LINE_AA)\n            else:  # needs_right_turn\n                robot.spinRight(0.4)\n                cv2.putText(depth_colormap, f'Searching right {closest_distance:.0f}mm', \n                           (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2, cv2.LINE_AA)\n            print(f'Love: Fast approach {closest_distance:.0f}mm', end='\\r')\n        \n        # Display occlusion info if multiple humans detected\n        if len(all_humans) > 1:\n            cv2.putText(frame, f'{len(all_humans)} people detected', \n                       (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n                \n    else:\n        # No human detected - stop and wait (occlusion or person left)\n        robot.stop()\n        if len(all_humans) == 0:\n            cv2.putText(depth_colormap, 'Waiting for person...', \n                       (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 2, cv2.LINE_AA)\n            print('Love: No person detected - waiting', end='\\r')\n        else:\n            cv2.putText(depth_colormap, 'Target occluded - waiting...', \n                       (160, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2, cv2.LINE_AA)\n            print('Love: Target occluded by other people', end='\\r')\n    \n    # Visualize collision detection zone on depth map\n    cv2.rectangle(depth_colormap, (168, 94), (504, 282), (0, 255, 0), 1)\n    \n    # Display obstacle warning if detected\n    if obstacle_detected:\n        cv2.circle(depth_colormap, (336, 188), 30, (0, 0, 255), 3)\n        cv2.putText(depth_colormap, f'OBS: {obstacle_distance:.0f}mm', \n                   (260, 220), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2, cv2.LINE_AA)\n    \n    # Display images\n    scale = 0.8\n    resized_color = cv2.resize(frame, None, fx=scale, fy=scale, \n                               interpolation=cv2.INTER_AREA)\n    resized_depth = cv2.resize(depth_colormap, None, fx=scale, fy=scale, \n                               interpolation=cv2.INTER_AREA)\n    display_color.value = bgr8_to_jpeg(resized_color)\n    display_depth.value = bgr8_to_jpeg(resized_depth)\n\n# Attach callback to camera\ncamera.observe(enhanced_follow_callback, names=['color_value'])",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[12/05/2025-10:21:22] [TRT] [I] Loaded engine size: 52 MiB\n,[12/05/2025-10:21:22] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n,[12/05/2025-10:21:22] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +36, now: CPU 1, GPU 84 (MiB)\n"
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": "**To stop the camera and robot:**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "camera.stop()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "**Behavior Description:**\n\nThe robot exhibits love behavior by following detected humans:\n\n1. **Distance Measurement**: Calculates distance to closest detected human using depth image\n2. **Intelligent Tracking**: \n   - Calculates human position relative to frame center\n   - Only turns when human is >15% off-center (avoids continuous adjustment)\n   - Turns left/right to recenter human in view\n3. **Distance-Based Approach**:\n   - **< 450mm**: Slight backward retreat (0.2 speed)\n   - **450-600mm**: Stop - optimal \"loving\" distance\n   - **600-1200mm**: Slow approach (0.3) or turn to track (0.3)\n   - **> 1200mm**: Fast approach (0.5) or turn to track (0.4)\n4. **Smart Following**: \n   - When centered: moves forward toward human\n   - When off-center: rotates to recenter before/while approaching\n   - When lost: stops and waits (no continuous searching)\n\nThe robot follows humans by maintaining visual tracking and optimal proximity, turning only when necessary to keep the human centered in its field of view.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}