{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "1ddd419e-12b6-4c1f-9935-d98ba98aa685",
      "cell_type": "code",
      "source": "# Part B: Person Following with Obstacle Avoidance\n# Combines Tutorial 3 Part B (depth-based collision avoidance) with Tutorial 5 (YOLO human detection)\n# Uses depth for reliable obstacle detection and YOLO only for tracking the target human\n\n\"\"\"\nSTEP 1: Display Widgets Setup\nBased on Tutorial 2 Part A\n\"\"\"\nimport ipywidgets.widgets as widgets\nfrom IPython.display import display\nimport cv2\n\ndisplay_color = widgets.Image(format='jpeg', width='45%')\ndisplay_depth = widgets.Image(format='jpeg', width='45%')\nlayout = widgets.Layout(width='100%')\n\nsidebyside = widgets.HBox([display_color, display_depth], layout=layout)\ndisplay(sidebyside)\n\ndef bgr8_to_jpeg(value):\n    return bytes(cv2.imencode('.jpg', value)[1])\n\n\n\"\"\"\nSTEP 2: Load YOLO Model\nBased on Tutorial 5\n\"\"\"\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11l_half.engine\")\n\n\n\"\"\"\nSTEP 3: Person Re-Identification System\nHSV histogram matching to track same person across frames (handles occlusion)\n\"\"\"\nimport numpy as np\n\nclass PersonTracker:\n    \"\"\"\n    Tracks a specific person using HSV histogram matching.\n    Handles occlusions by re-identifying based on appearance.\n    \"\"\"\n    \n    def __init__(self, similarity_threshold=0.6, h_bins=16, s_bins=16):\n        self.target_histogram = None\n        self.target_locked = False\n        self.similarity_threshold = similarity_threshold\n        self.h_bins = h_bins\n        self.s_bins = s_bins\n        self.frames_lost = 0\n        self.max_lost_frames = 30  # ~1 second at 30fps\n    \n    def compute_histogram(self, roi_bgr):\n        \"\"\"Compute normalized HSV histogram for region of interest.\"\"\"\n        if roi_bgr.size == 0:\n            return None\n        \n        hsv = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2HSV)\n        hist = cv2.calcHist([hsv], [0, 1], None,\n                           [self.h_bins, self.s_bins],\n                           [0, 180, 0, 256])\n        cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)\n        return hist\n    \n    def lock_target(self, frame, bbox):\n        \"\"\"Lock onto specific person by storing their histogram.\"\"\"\n        x1, y1, x2, y2 = map(int, bbox)\n        roi = frame[y1:y2, x1:x2]\n        \n        self.target_histogram = self.compute_histogram(roi)\n        if self.target_histogram is not None:\n            self.target_locked = True\n            self.frames_lost = 0\n            return True\n        return False\n    \n    def find_target(self, frame, detections):\n        \"\"\"\n        Find target person among all detections using histogram matching.\n        Returns (found, bbox, distance) tuple.\n        \"\"\"\n        if not self.target_locked:\n            return False, None, float('inf')\n        \n        best_match_score = -1\n        best_bbox = None\n        best_distance = float('inf')\n        \n        for detection in detections:\n            bbox, distance = detection\n            x1, y1, x2, y2 = map(int, bbox)\n            roi = frame[y1:y2, x1:x2]\n            \n            hist = self.compute_histogram(roi)\n            if hist is None:\n                continue\n            \n            # Calculate histogram similarity\n            similarity = cv2.compareHist(self.target_histogram, hist, \n                                        cv2.HISTCMP_CORREL)\n            \n            if similarity > best_match_score:\n                best_match_score = similarity\n                best_bbox = bbox\n                best_distance = distance\n        \n        if best_match_score > self.similarity_threshold:\n            self.frames_lost = 0\n            return True, best_bbox, best_distance\n        else:\n            self.frames_lost += 1\n            if self.frames_lost > self.max_lost_frames:\n                self.target_locked = False\n            return False, None, float('inf')\n    \n    def reset(self):\n        \"\"\"Reset tracker to lock onto new person.\"\"\"\n        self.target_histogram = None\n        self.target_locked = False\n        self.frames_lost = 0\n\n\n\"\"\"\nSTEP 4: Human Detection with YOLO\nBased on Tutorial 5 - detect all humans and measure distances\n\"\"\"\n\ndef detect_all_humans(frame, depth_image, conf_threshold=0.5):\n    \"\"\"\n    Detect all humans in frame using YOLO and calculate distances.\n    Returns list of (bbox, distance) tuples.\n    \"\"\"\n    results = model(frame, verbose=False)\n    detections = []\n    \n    for result in results:\n        for i in range(len(result.boxes.cls)):\n            if result.boxes.cls[i] == 0:  # Human class only\n                if result.boxes.conf[i] > conf_threshold:\n                    bbox = result.boxes.xyxy[i].cpu().numpy()\n                    \n                    # Calculate center of bounding box\n                    center_x = int((bbox[0] + bbox[2]) / 2)\n                    center_y = int((bbox[1] + bbox[3]) / 2)\n                    \n                    # Get distance from depth image\n                    if (0 <= center_y < depth_image.shape[0] and \n                        0 <= center_x < depth_image.shape[1]):\n                        distance = depth_image[center_y, center_x]\n                        \n                        if not np.isnan(distance) and distance > 0:\n                            detections.append((bbox, distance))\n    \n    return detections\n\n\n\"\"\"\nSTEP 5: Depth-Based Obstacle Detection\nBased on Tutorial 3 Part B - uses depth image for reliable obstacle detection\nThis detects ALL obstacles (people, furniture, walls) in central region\n\"\"\"\n\ndef detect_obstacle_depth(depth_image):\n    \"\"\"\n    Detect obstacles in robot's path using depth image.\n    Based on Tutorial 3 Part B collision avoidance method.\n    \n    Returns (obstacle_detected, min_distance, obstacle_region)\n    - obstacle_region: 'left', 'right', or 'center'\n    \"\"\"\n    # Copy and clean depth image\n    depth_test = depth_image.copy()\n    depth_test = np.nan_to_num(depth_test, nan=0.0).astype(np.float32)\n    \n    # Define central area (robot's path) - Tutorial 3 Part B approach\n    # Adjust these values based on your robot's configuration\n    depth_test[:94, :] = 0      # Remove top area\n    depth_test[282:, :] = 0     # Remove bottom area\n    depth_test[:, :168] = 0     # Remove left side\n    depth_test[:, 504:] = 0     # Remove right side\n    \n    # Filter depth values (remove noise and distant objects)\n    depth_test[depth_test < 100] = 0    # Too close readings (noise)\n    depth_test[depth_test > 1000] = 0   # Too far (not immediate threat)\n    \n    # Check if any obstacle in path\n    if depth_test.max() == 0:\n        return False, float('inf'), 'none'\n    \n    # Find minimum distance (closest obstacle)\n    min_distance = depth_test[depth_test != 0].min()\n    \n    # Determine which region obstacle is in (left/center/right)\n    # Split central area into three regions\n    height, width = depth_test.shape\n    left_region = depth_test[:, 168:280]\n    center_region = depth_test[:, 280:392]\n    right_region = depth_test[:, 392:504]\n    \n    # Find which region has the closest obstacle\n    left_min = left_region[left_region != 0].min() if left_region[left_region != 0].size > 0 else float('inf')\n    center_min = center_region[center_region != 0].min() if center_region[center_region != 0].size > 0 else float('inf')\n    right_min = right_region[right_region != 0].min() if right_region[right_region != 0].size > 0 else float('inf')\n    \n    if center_min <= left_min and center_min <= right_min:\n        obstacle_region = 'center'\n    elif left_min < right_min:\n        obstacle_region = 'left'\n    else:\n        obstacle_region = 'right'\n    \n    return True, min_distance, obstacle_region\n\n\n\"\"\"\nSTEP 6: Camera System Initialization\nBased on Tutorial 2 Part B and Tutorial 4 Part A\n\"\"\"\nimport traitlets\nimport pyzed.sl as sl\nimport threading\nimport motors\nfrom traitlets.config.configurable import SingletonConfigurable\n\nrobot = motors.MotorsYukon(mecanum=False)\n\nclass Camera(SingletonConfigurable):\n    color_value = traitlets.Any()\n    \n    def __init__(self):\n        super(Camera, self).__init__()\n        \n        self.zed = sl.Camera()\n        init_params = sl.InitParameters()\n        init_params.camera_resolution = sl.RESOLUTION.VGA\n        init_params.depth_mode = sl.DEPTH_MODE.ULTRA\n        init_params.coordinate_units = sl.UNIT.MILLIMETER\n        \n        status = self.zed.open(init_params)\n        if status != sl.ERROR_CODE.SUCCESS:\n            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n            self.zed.close()\n            exit(1)\n        \n        self.runtime = sl.RuntimeParameters()\n        self.thread_runnning_flag = False\n        \n        camera_info = self.zed.get_camera_information()\n        self.width = camera_info.camera_configuration.resolution.width\n        self.height = camera_info.camera_configuration.resolution.height\n        self.image = sl.Mat(self.width, self.height, sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n        self.depth = sl.Mat(self.width, self.height, sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n    \n    def _capture_frames(self):\n        while self.thread_runnning_flag:\n            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n                self.color_value_BGRA = self.image.get_data()\n                self.color_value = cv2.cvtColor(self.color_value_BGRA, cv2.COLOR_BGRA2BGR)\n                self.depth_image = np.asanyarray(self.depth.get_data())\n    \n    def start(self):\n        if not self.thread_runnning_flag:\n            self.thread_runnning_flag = True\n            self.thread = threading.Thread(target=self._capture_frames)\n            self.thread.start()\n    \n    def stop(self):\n        if self.thread_runnning_flag:\n            self.thread_runnning_flag = False\n            self.thread.join()\n            robot.stop()\n\ncamera = Camera()\ncamera.start()\n\n\n\"\"\"\nSTEP 7: Following Behavior with Differential Drive\nSmooth curved movement using individual wheel speed control\n\"\"\"\n\ndef follow_target(frame, target_bbox, target_distance, depth_colormap):\n    \"\"\"\n    Follow target person while maintaining optimal distance.\n    Uses differential drive for smooth curves (love_detection.ipynb approach).\n    \"\"\"\n    # Calculate target position relative to center\n    bbox_center_x = (target_bbox[0] + target_bbox[2]) / 2\n    frame_center_x = camera.width / 2\n    \n    # Calculate angle offset normalized to [-1, 1]\n    offset_x = bbox_center_x - frame_center_x\n    max_offset = camera.width / 2\n    normalized_angle = offset_x / max_offset\n    \n    # Convert to radians for differential drive\n    angle_rad = normalized_angle * (np.pi / 4)\n    turning_factor = np.cos(angle_rad)\n    \n    # Draw target box\n    cv2.rectangle(frame, \n                  (int(target_bbox[0]), int(target_bbox[1])), \n                  (int(target_bbox[2]), int(target_bbox[3])), \n                  (0, 255, 0), 3)\n    \n    # Distance-based behavior\n    if target_distance < 600:\n        robot.backward(0.2)\n        cv2.putText(depth_colormap, f'TOO CLOSE {target_distance:.0f}mm', \n                   (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n        \n    elif 600 <= target_distance <= 900:\n        robot.stop()\n        cv2.putText(depth_colormap, f'OPTIMAL {target_distance:.0f}mm', \n                   (200, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n        \n    elif 900 < target_distance <= 1500:\n        base_speed = 0.3\n        \n        if angle_rad > 0.1:  # Right\n            robot.frontLeft(speed=base_speed)\n            robot.backLeft(speed=base_speed)\n            robot.frontRight(speed=base_speed * turning_factor)\n            robot.backRight(speed=base_speed * turning_factor)\n            cv2.putText(depth_colormap, f'FOLLOW RIGHT {target_distance:.0f}mm', \n                       (160, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 180, 0), 2)\n            \n        elif angle_rad < -0.1:  # Left\n            robot.frontLeft(speed=base_speed * turning_factor)\n            robot.backLeft(speed=base_speed * turning_factor)\n            robot.frontRight(speed=base_speed)\n            robot.backRight(speed=base_speed)\n            cv2.putText(depth_colormap, f'FOLLOW LEFT {target_distance:.0f}mm', \n                       (160, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 180, 0), 2)\n            \n        else:  # Centered\n            robot.forward(base_speed)\n            cv2.putText(depth_colormap, f'APPROACH {target_distance:.0f}mm', \n                       (200, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 180, 0), 2)\n            \n    else:  # > 1500mm\n        base_speed = 0.5\n        \n        if angle_rad > 0.1:\n            robot.frontLeft(speed=base_speed)\n            robot.backLeft(speed=base_speed)\n            robot.frontRight(speed=base_speed * turning_factor)\n            robot.backRight(speed=base_speed * turning_factor)\n            cv2.putText(depth_colormap, f'CHASE RIGHT {target_distance:.0f}mm', \n                       (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2)\n            \n        elif angle_rad < -0.1:\n            robot.frontLeft(speed=base_speed * turning_factor)\n            robot.backLeft(speed=base_speed * turning_factor)\n            robot.frontRight(speed=base_speed)\n            robot.backRight(speed=base_speed)\n            cv2.putText(depth_colormap, f'CHASE LEFT {target_distance:.0f}mm', \n                       (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2)\n            \n        else:\n            robot.forward(base_speed)\n            cv2.putText(depth_colormap, f'CHASE {target_distance:.0f}mm', \n                       (220, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2)\n\n\n\"\"\"\nSTEP 8: Obstacle Avoidance with Navigation\nBased on Tutorial 3 Part B approach - navigate around obstacles\n\"\"\"\n\ndef navigate_around_obstacle(obstacle_region, obstacle_distance, depth_colormap):\n    \"\"\"\n    Navigate around obstacle based on Tutorial 3 Part B.\n    Turns away from obstacle until path is clear.\n    \n    Args:\n        obstacle_region: 'left', 'center', or 'right'\n        obstacle_distance: distance to closest obstacle in mm\n    \"\"\"\n    # Critical distance threshold (Tutorial 3 Part B uses 400mm)\n    if obstacle_distance < 400:\n        # Very close - stop or back up\n        robot.backward(0.2)\n        cv2.putText(depth_colormap, f'EMERGENCY STOP {obstacle_distance:.0f}mm!', \n                   (120, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n    else:\n        # Navigate around based on obstacle position\n        if obstacle_region == 'left':\n            # Obstacle on left, turn right\n            robot.right(0.4)\n            cv2.putText(depth_colormap, f'AVOID LEFT {obstacle_distance:.0f}mm', \n                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n            \n        elif obstacle_region == 'right':\n            # Obstacle on right, turn left\n            robot.left(0.4)\n            cv2.putText(depth_colormap, f'AVOID RIGHT {obstacle_distance:.0f}mm', \n                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n            \n        else:  # center\n            # Obstacle ahead, turn right (default direction)\n            robot.right(0.4)\n            cv2.putText(depth_colormap, f'AVOID CENTER {obstacle_distance:.0f}mm', \n                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n\n\n\"\"\"\nSTEP 9: Main Control Loop with State Machine\nCombines target following with obstacle avoidance\n\"\"\"\n\ntracker = PersonTracker(similarity_threshold=0.6)\ncurrent_state = 'idle'  # States: 'idle', 'locking', 'following', 'avoiding'\n\n# Keyboard input (Answer_keyboard_control.ipynb pattern)\ntext_input = widgets.Text(\n    value='',\n    placeholder='Type: l=Lock target, s=Stop, r=Reset',\n    description='Control:',\n    disabled=False\n)\n\ndef on_text_change(change):\n    \"\"\"Handle keyboard commands.\"\"\"\n    global current_state\n    \n    input_value = change['new']\n    \n    if len(input_value) > 0:\n        last_char = input_value[-1].lower()\n        \n        if last_char == 'l':\n            current_state = 'locking'\n            print('\\nLOCKING - Will lock onto closest person')\n            \n        elif last_char == 's':\n            current_state = 'idle'\n            tracker.reset()\n            robot.stop()\n            print('\\nSTOPPED - Robot halted, tracker reset')\n            \n        elif last_char == 'r':\n            tracker.reset()\n            current_state = 'idle'\n            robot.stop()\n            print('\\nRESET - Tracker reset')\n\ntext_input.observe(on_text_change, names='value')\ndisplay(text_input)\n\n\ndef main_callback(change):\n    \"\"\"\n    Main control callback - state machine for following and avoidance.\n    Combines YOLO human tracking with depth-based obstacle avoidance.\n    \"\"\"\n    global current_state\n    \n    frame = change['new'].copy()\n    depth_image = camera.depth_image\n    \n    # Create depth colormap with obstacle detection visualization\n    depth_test = depth_image.copy()\n    depth_test = np.nan_to_num(depth_test, nan=0.0).astype(np.float32)\n    \n    # Show detection zone (Tutorial 3 Part B approach)\n    depth_test[:94, :] = 0\n    depth_test[282:, :] = 0\n    depth_test[:, :168] = 0\n    depth_test[:, 504:] = 0\n    depth_test[depth_test < 100] = 0\n    depth_test[depth_test > 1000] = 0\n    \n    depth_colormap = cv2.applyColorMap(\n        cv2.convertScaleAbs(depth_test, alpha=0.03), \n        cv2.COLORMAP_JET\n    )\n    \n    # Detect all humans using YOLO\n    all_detections = detect_all_humans(frame, depth_image)\n    \n    # Draw all detected humans in blue\n    for bbox, distance in all_detections:\n        cv2.rectangle(frame, \n                     (int(bbox[0]), int(bbox[1])), \n                     (int(bbox[2]), int(bbox[3])), \n                     (255, 0, 0), 2)\n    \n    # Detect obstacles using depth (Tutorial 3 Part B method)\n    obstacle_detected, obstacle_distance, obstacle_region = detect_obstacle_depth(depth_image)\n    \n    # State machine\n    if current_state == 'idle':\n        robot.stop()\n        cv2.putText(frame, 'IDLE - Press L to lock target', \n                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n        cv2.putText(depth_colormap, 'IDLE', \n                   (270, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 2)\n    \n    elif current_state == 'locking':\n        if len(all_detections) > 0:\n            # Lock onto closest person\n            closest_detection = min(all_detections, key=lambda x: x[1])\n            closest_bbox, closest_distance = closest_detection\n            \n            if tracker.lock_target(frame, closest_bbox):\n                current_state = 'following'\n                print(f'\\nTARGET LOCKED at {closest_distance:.0f}mm')\n                \n                cv2.rectangle(frame, \n                            (int(closest_bbox[0]), int(closest_bbox[1])), \n                            (int(closest_bbox[2]), int(closest_bbox[3])), \n                            (0, 255, 0), 3)\n        else:\n            cv2.putText(frame, 'LOCKING - No person detected', \n                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)\n    \n    elif current_state == 'following':\n        # Find target person using YOLO + histogram matching\n        target_found, target_bbox, target_distance = tracker.find_target(frame, all_detections)\n        \n        if target_found:\n            # Check for obstacles using depth (more reliable than YOLO for obstacles)\n            if obstacle_detected and obstacle_distance < 700:  # 700mm safety threshold\n                # Obstacle in path - switch to avoiding\n                current_state = 'avoiding'\n                cv2.putText(frame, f'OBSTACLE DETECTED - AVOIDING', \n                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n            else:\n                # Path clear - follow target\n                follow_target(frame, target_bbox, target_distance, depth_colormap)\n                cv2.putText(frame, f'FOLLOWING - Distance: {target_distance:.0f}mm', \n                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n        else:\n            # Lost target - search by rotating\n            robot.spinRight(0.3)\n            cv2.putText(frame, f'SEARCHING ({tracker.frames_lost} frames)', \n                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)\n            cv2.putText(depth_colormap, 'SEARCHING', \n                       (220, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2)\n            \n            if tracker.frames_lost > tracker.max_lost_frames:\n                current_state = 'idle'\n                robot.stop()\n                print('\\nTARGET LOST - Returning to idle')\n    \n    elif current_state == 'avoiding':\n        # Try to maintain visual on target while avoiding\n        target_found, target_bbox, target_distance = tracker.find_target(frame, all_detections)\n        \n        if target_found:\n            # Draw target even while avoiding\n            cv2.rectangle(frame, \n                         (int(target_bbox[0]), int(target_bbox[1])), \n                         (int(target_bbox[2]), int(target_bbox[3])), \n                         (0, 255, 0), 3)\n        \n        # Check if obstacle is still blocking\n        if obstacle_detected and obstacle_distance < 700:\n            # Still blocked - navigate around\n            navigate_around_obstacle(obstacle_region, obstacle_distance, depth_colormap)\n            cv2.putText(frame, f'AVOIDING - Obstacle {obstacle_distance:.0f}mm {obstacle_region.upper()}', \n                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n        else:\n            # Path clear - check if target is still visible\n            if target_found:\n                current_state = 'following'\n                print('\\nPath clear - Resuming following')\n            else:\n                # Lost target during avoidance\n                robot.stop()\n                current_state = 'idle'\n                print('\\nTarget lost during avoidance')\n    \n    # Display status\n    cv2.putText(frame, f'State: {current_state.upper()}', \n               (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n    if obstacle_detected:\n        cv2.putText(frame, f'Obstacle: {obstacle_distance:.0f}mm {obstacle_region}', \n                   (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n    \n    # Display images\n    scale = 0.15\n    resized_color = cv2.resize(frame, None, fx=scale, fy=scale, \n                               interpolation=cv2.INTER_AREA)\n    resized_depth = cv2.resize(depth_colormap, None, fx=scale, fy=scale, \n                               interpolation=cv2.INTER_AREA)\n    display_color.value = bgr8_to_jpeg(resized_color)\n    display_depth.value = bgr8_to_jpeg(resized_depth)\n\ncamera.observe(main_callback, names=['color_value'])\n\nprint('\\n=== Part B: Person Following with Obstacle Avoidance ===')\nprint('Commands:')\nprint('  l = Lock onto closest person and start following')\nprint('  s = Stop robot and reset tracker')\nprint('  r = Reset tracker only')\nprint('\\nFeatures:')\nprint('  - YOLO for target human tracking with re-identification')\nprint('  - Depth-based obstacle detection (Tutorial 3 Part B method)')\nprint('  - Navigates around obstacles while maintaining target lock')\nprint('  - Smooth differential drive movement')\nprint('  - Maintains optimal distance (600-900mm)')\nprint('========================================================')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}