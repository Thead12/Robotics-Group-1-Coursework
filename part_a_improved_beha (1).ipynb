{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808ecdd4",
   "metadata": {},
   "source": [
    "# Part A\n",
    "\n",
    "- **Love**: Follow and maintain optimal distance\n",
    "- **Fear**: Retreat from detected humans\n",
    "- **Aggressive**: Relentless pursuit\n",
    "- **Curious**: Cautious investigation with circling\n",
    "\n",
    "Keyboard controls:\n",
    "- 'l' = Love behavior\n",
    "- 'f' = Fear behavior  \n",
    "- 'a' = Aggressive behavior\n",
    "- 'c' = Curious behavior\n",
    "- 's' = Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368a2d73-1df4-48e1-8526-56a59e8fc697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-05 09:49:17--  https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/latest/gesture_recognizer.task\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 2a00:1450:4009:c0b::cf, 2a00:1450:4009:c17::cf, 2a00:1450:4009:c13::cf, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|2a00:1450:4009:c0b::cf|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 8373440 (8.0M) [application/octet-stream]\n",
      "Saving to: â€˜gesture_recognizer.task.10â€™\n",
      "\n",
      "gesture_recognizer. 100%[===================>]   7.99M  11.7MB/s    in 0.7s    \n",
      "\n",
      "2025-12-05 09:49:17 (11.7 MB/s) - â€˜gesture_recognizer.task.10â€™ saved [8373440/8373440]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/latest/gesture_recognizer.task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa5062",
   "metadata": {},
   "source": [
    "**Step 1: Display Widgets**\n",
    "\n",
    "Standard widget setup for displaying color and depth images side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5590a5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee8e8ef901d4f76bb8d2fe335959086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')), laâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "\n",
    "# Create two widgets for displaying images\n",
    "display_color = widgets.Image(format='jpeg', width='45%')\n",
    "display_depth = widgets.Image(format='jpeg', width='45%')\n",
    "layout = widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth], layout=layout)\n",
    "display(sidebyside)\n",
    "\n",
    "# convert numpy array to jpeg for display\n",
    "def bgr8_to_jpeg(value):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f4b02",
   "metadata": {},
   "source": [
    "**Step 2: Load YOLO Model**\n",
    "\n",
    "load the TensorRT YOLO model for human detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "244ac057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo11l_half.engine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c41561-a052-4d67-8726-ff27b1ed7b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mediapipe in /home/robotics/.local/lib/python3.10/site-packages (0.10.18)\n",
      "Requirement already satisfied: absl-py in /home/robotics/.local/lib/python3.10/site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /home/robotics/.local/lib/python3.10/site-packages (from mediapipe) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece in /home/robotics/.local/lib/python3.10/site-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: jax in /home/robotics/.local/lib/python3.10/site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: numpy<2 in /home/robotics/.local/lib/python3.10/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (25.1.0)\n",
      "Requirement already satisfied: opencv-contrib-python in /home/robotics/.local/lib/python3.10/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /home/robotics/.local/lib/python3.10/site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: jaxlib in /home/robotics/.local/lib/python3.10/site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (from mediapipe) (3.5.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: opt_einsum in /home/robotics/.local/lib/python3.10/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.12 in /home/robotics/.local/lib/python3.10/site-packages (from jax->mediapipe) (1.15.3)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /home/robotics/.local/lib/python3.10/site-packages (from jax->mediapipe) (0.5.3)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d493dfb5",
   "metadata": {},
   "source": [
    "**Step 3: Camera System Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c98ab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-05 09:49:27 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-12-05 09:49:27 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-12-05 09:49:27 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-12-05 09:49:27 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-12-05 09:49:28 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-12-05 09:49:28 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-12-05 09:49:28 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-12-05 09:49:28 UTC][ZED][INFO] [Init]  Serial Number: S/N 36955685\n",
      "Loading yolo11l_half.engine for TensorRT inference...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['numpy==1.23.5'] not found, attempting AutoUpdate...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy==1.23.5\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (13.9 MB)\n",
      "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13.9/13.9 MB 10.4 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-1.23.5\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 10.0s, installed 1 package: ['numpy==1.23.5']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m âš ï¸ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "Aggression: Checking for target (speed: 0.8)"
     ]
    }
   ],
   "source": [
    "import traitlets\n",
    "import numpy as np\n",
    "import pyzed.sl as sl\n",
    "import threading\n",
    "import motors\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "# NEW: MediaPipe Gesture Recognition imports\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# NEW: MediaPipe classes\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "GestureRecognizer = vision.GestureRecognizer\n",
    "GestureRecognizerOptions = vision.GestureRecognizerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Initialize robot motor control\n",
    "robot = motors.MotorsYukon(mecanum=False)\n",
    "\n",
    "# Camera class with traitlets (SAME AS BEFORE)\n",
    "class Camera(SingletonConfigurable):\n",
    "    color_value = traitlets.Any()\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "\n",
    "        # ZED camera initialization\n",
    "        self.zed = sl.Camera()\n",
    "        init_params = sl.InitParameters()\n",
    "        init_params.camera_resolution = sl.RESOLUTION.VGA\n",
    "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "        init_params.coordinate_units = sl.UNIT.MILLIMETER\n",
    "\n",
    "        status = self.zed.open(init_params)\n",
    "        if status != sl.ERROR_CODE.SUCCESS:\n",
    "            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "        self.thread_runnning_flag = False\n",
    "\n",
    "        camera_info = self.zed.get_camera_information()\n",
    "        self.width = camera_info.camera_configuration.resolution.width\n",
    "        self.height = camera_info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(self.width, self.height, sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(self.width, self.height, sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "        self.point_cloud = sl.Mat(self.width, self.height, sl.MAT_TYPE.F32_C4, sl.MEM.CPU)\n",
    "\n",
    "    def _capture_frames(self):\n",
    "        while(self.thread_runnning_flag == True):\n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "                \n",
    "                self.color_value_BGRA = self.image.get_data()\n",
    "                self.color_value = cv2.cvtColor(self.color_value_BGRA, cv2.COLOR_BGRA2BGR)\n",
    "                self.depth_image = np.asanyarray(self.depth.get_data())\n",
    "\n",
    "    def start(self):\n",
    "        if self.thread_runnning_flag == False:\n",
    "            self.thread_runnning_flag = True\n",
    "            self.thread = threading.Thread(target=self._capture_frames)\n",
    "            self.thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False\n",
    "            self.thread.join()\n",
    "            robot.stop()\n",
    "\n",
    "# Initialize and start camera\n",
    "camera = Camera()\n",
    "camera.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae9b1c",
   "metadata": {},
   "source": [
    "**Step 4: Behavior Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1750633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable for current behavior mode\n",
    "current_behavior = 'stop'\n",
    "\n",
    "\n",
    "def detect_human(frame):\n",
    "    \"\"\"\n",
    "    Human detection and distance calculation\n",
    "    \n",
    "    From Tutorial 5:\n",
    "    - YOLO detection for humans (class 0)\n",
    "    - Bounding box extraction: result.boxes.xyxy[i]\n",
    "    - Center calculation: (bbox[0] + bbox[2]) / 2 for x, (bbox[1] + bbox[3]) / 2 for y\n",
    "    - Distance extraction: camera.depth_image[center_y, center_x]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    results = model(frame, verbose=False)\n",
    "    \n",
    "    # confidence threshold for detections\n",
    "    conf_threshold = 0.5\n",
    "    human_detected = False\n",
    "    min_distance = float('inf')\n",
    "    closest_bbox = None\n",
    "    \n",
    "    # iterate through detections\n",
    "    for result in results:\n",
    "        for i in range(len(result.boxes.cls)):\n",
    "            if result.boxes.cls[i] == 0:\n",
    "                # Check confidence (Tutorial 5)\n",
    "                if result.boxes.conf[i] > conf_threshold:\n",
    "                    human_detected = True\n",
    "                    # Get bounding box coordinates (Tutorial 5)\n",
    "                    bbox = result.boxes.xyxy[i]\n",
    "                    \n",
    "                    # Calculate center of bounding box (Tutorial 5 method)\n",
    "                    # bbox format: [x1, y1, x2, y2]\n",
    "                    center_x = int((bbox[0] + bbox[2]) / 2)\n",
    "                    center_y = int((bbox[1] + bbox[3]) / 2)\n",
    "                    \n",
    "                    # Get depth at human center (Tutorial 5)\n",
    "                    # Depth image indexing: depth_image[row, col] or depth_image[y, x]\n",
    "                    # Returns distance in millimeters\n",
    "                    if (0 <= center_y < camera.height and 0 <= center_x < camera.width):\n",
    "                        distance = camera.depth_image[center_y, center_x]\n",
    "                        \n",
    "                        # NaN means no depth data, negative or zero means invalid\n",
    "                        if not np.isnan(distance) and distance > 0:\n",
    "                            # Track closest human\n",
    "                            if distance < min_distance:\n",
    "                                min_distance = distance\n",
    "                                closest_bbox = bbox\n",
    "    \n",
    "    return human_detected, min_distance, closest_bbox\n",
    "\n",
    "def love_behavior(frame, depth_colormap, human_detected, distance, bbox):\n",
    "    \"\"\"\n",
    "    Love behavior - follow human and maintain comfortable distance\n",
    "    \n",
    "    Distance thresholds chosen based on testing:\n",
    "    - < 450mm: Too close, back up\n",
    "    - 450-600mm: Optimal distance, stop\n",
    "    - 600-1200mm: Medium range, approach slowly\n",
    "    - > 1200mm: Far away, approach faster\n",
    "    \n",
    "    \"\"\"\n",
    "    if human_detected and distance != float('inf'):\n",
    "        # Draw dectection box(Tutorial 5 pattern)\n",
    "        cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), \n",
    "                     (int(bbox[2]), int(bbox[3])), (255, 105, 180), 2)\n",
    "        \n",
    "         # Calculate if human is left/right of center (Tutorial 5 approach)\n",
    "        bbox_center_x = (bbox[0] + bbox[2]) / 2\n",
    "        frame_center_x = camera.width / 2\n",
    "\n",
    "        # Distance-based behavior (new thresholds for love behavior)\n",
    "        if distance < 450:\n",
    "            robot.backward(0.5)\n",
    "            cv2.putText(depth_colormap, f'LOVE: Too close {distance:.0f}mm', \n",
    "                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "        elif distance >= 450 and distance <= 600:\n",
    "            robot.stop()\n",
    "            cv2.putText(depth_colormap, f'LOVE: Perfect {distance:.0f}mm <3', \n",
    "                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)\n",
    "        elif distance > 600 and distance <= 1200:\n",
    "            # Medium distance - turn to center, then approach\n",
    "            # 50 pixel threshold to avoid constant adjustment\n",
    "            if bbox_center_x < frame_center_x - 50:\n",
    "                robot.left(0.3) \n",
    "            elif bbox_center_x > frame_center_x + 50:\n",
    "                robot.right(0.3) \n",
    "            else:\n",
    "                robot.forward(0.3) \n",
    "            cv2.putText(depth_colormap, f'LOVE: Approaching {distance:.0f}mm', \n",
    "                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 180, 0), 2)\n",
    "        else:  # distance > 1200\n",
    "            # Far away - approach faster with turning\n",
    "            if bbox_center_x < frame_center_x - 50:\n",
    "                robot.left(0.4)\n",
    "            elif bbox_center_x > frame_center_x + 50:\n",
    "                robot.right(0.4)\n",
    "            else:\n",
    "                robot.forward(0.5)\n",
    "            cv2.putText(depth_colormap, f'LOVE: Coming {distance:.0f}mm', \n",
    "                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2)\n",
    "    else:\n",
    "        # No human detected - stop and wait\n",
    "        robot.stop()\n",
    "        cv2.putText(depth_colormap, 'LOVE: Waiting...', \n",
    "                   (200, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 2)\n",
    "\n",
    "# Global for fear like a mouse or rabbit\n",
    "# escape zone\n",
    "escape_start = None\n",
    "escape_direction = None\n",
    "escape_phase = None\n",
    "\n",
    "def fear_behavior(frame, depth_colormap, human_detected, distance, bbox):\n",
    "    \"\"\"\n",
    "    Fear behavior - retreat from humans and runs away \n",
    "    \n",
    "    Distance thresholds:\n",
    "    - < 500mm: DANGER - fast retreat (0.6 speed)\n",
    "    - < 900mm: CAUTION - medium retreat (0.4 speed)\n",
    "    - < 1300mm: ALERT - slow retreat (0.2 speed)\n",
    "    - >= 1300mm: SAFE - stop retreating\n",
    "\n",
    "    robot will reach a certain once safw, turn 90 degree and run for 5 sec \n",
    "    \"\"\"\n",
    "    global escape_direction, escape_phase, escape_start\n",
    "\n",
    "    if human_detected and distance != float('inf'):\n",
    "        # Draw red warning box\n",
    "        cv2.rectangle( frame, (int(bbox[0]), int(bbox[1])),\n",
    "                      (int(bbox[2]), int(bbox[3])), (0, 0, 255), 2)\n",
    "\n",
    "        # Graduated fear respone\n",
    "        if distance < 500:\n",
    "            # Very close fast retreat\n",
    "            escape_phase = 'retreating'\n",
    "            escape_start = None\n",
    "            robot.backward(0.6)\n",
    "            cv2.putText(depth_colormap, f'Fear: DANGER {distance:.0f}mm!',\n",
    "                        (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "        elif distance < 900:\n",
    "            #Medium distance medium retract\n",
    "            escape_phase = 'retreating'\n",
    "            escape_start = None\n",
    "            robot.backward(0.4)\n",
    "            cv2.putText(depth_colormap, f'Fear: Caution {distance:.0f}mm',\n",
    "                        (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2)\n",
    "        elif distance < 1300:\n",
    "            # Far but caution, slow retreat\n",
    "            escape_phase = 'retreating'\n",
    "            escape_start = None\n",
    "            robot.backward(0.2)\n",
    "            cv2.putText(depth_colormap, f'Fear: Alert {distance:.0f}mm',\n",
    "                        (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "        else:\n",
    "            # safe distance reached\n",
    "            if escape_phase == 'retreating':\n",
    "                # will start to run away\n",
    "                escape_phase = 'turning'\n",
    "                escape_start = time.time()\n",
    "                escape_direction = random.choice(['left', 'right'])\n",
    "            \n",
    "            if escape_phase == 'turning':\n",
    "                # turn 90 degrees\n",
    "                turn_duration = 1.0\n",
    "\n",
    "                if time.time() - escape_start < turn_duration:\n",
    "                    if escape_direction == 'left': # make turn\n",
    "                        robot.left(0.5)\n",
    "                        cv2.putText(depth_colormap, f'Fear : Turning left',\n",
    "                                    (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "                    else:\n",
    "                        robot.right(0.5)\n",
    "                        cv2.putText(depth_colormap, f'Fear : Turning right',\n",
    "                                    (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "                else:\n",
    "                    # will start to run away\n",
    "                    escape_phase = 'running'\n",
    "                    escape_start = time.time()\n",
    "            \n",
    "            elif escape_phase == 'running':\n",
    "                runaway = 5.0 # will run for 5 secs\n",
    "                if time.time() - escape_start < runaway:\n",
    "                    robot.forward(0.6)\n",
    "                    remaining = runaway - (time.time() - escape_start)\n",
    "                    cv2.putText(depth_colormap, f'Fear : running or going to hide {remaining:.1f}s',\n",
    "                                    (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)\n",
    "                else: # will stop and restrat normal action\n",
    "                    robot.stop()\n",
    "                    escape_phase = None\n",
    "                    escape_start = None\n",
    "                    cv2.putText(depth_colormap, f'Fear : I am safe from target',\n",
    "                                    (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            \n",
    "            else: # robot is safe\n",
    "                robot.stop()\n",
    "                cv2.putText(depth_colormap, f'Fear : safe{distance:.0f}mm',\n",
    "                                    (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "    else: \n",
    "        # no threat but escape in progress\n",
    "        if escape_phase == 'running':\n",
    "            runaway = 5.0\n",
    "            if time.time() - escape_start < runaway:\n",
    "                # will keep runnning even if target is yet to be seen\n",
    "                robot.forward(0.6)\n",
    "                remaining = runaway - (time.time() - escape_start)\n",
    "                cv2.putText(depth_colormap, f'Fear : running or going to hide {remaining:.1f}s',\n",
    "                                    (200, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)\n",
    "            else: \n",
    "                # wont run\n",
    "                robot.stop()\n",
    "                escape_phase = None\n",
    "                cv2.putText(depth_colormap, f'Fear : no threat',\n",
    "                                    (2000, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255, 0), 2)\n",
    "            \n",
    "        elif escape_phase == 'turning':\n",
    "            #will turn even target is not around\n",
    "            turn_duration = 1.0\n",
    "            if time.time() - escape_start < turn_duration:\n",
    "                if escape_direction == 'left':\n",
    "                    robot.left(0.5)\n",
    "                else:\n",
    "                    robot.right(0.5)\n",
    "            else: # start running\n",
    "                escape_phase = 'running'\n",
    "                escape_start = time.time()\n",
    "\n",
    "        else: \n",
    "            # No threat detected\n",
    "            robot.stop()\n",
    "            cv2.putText(depth_colormap, f'Fear: No threat ',\n",
    "                        (200, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "# Global for aggression like a dog and a bull\n",
    "dog_movement = { 'active':False, \n",
    "                'rush': 0, \n",
    "                'charging':None, \n",
    "                'barking':0} # similar why a dog barks at people and tire to attack\n",
    "\n",
    "def aggressive_behavior(frame, depth_colormap, human_detected, distance, bbox):\n",
    "    \"\"\"\n",
    "    Aggressive behavior - relentlessly push toward humans without stopping\n",
    "    - NEVER stops when target detected (shows more aggression)\n",
    "    - Speed scales with distance (closer = more intense)\n",
    "    - Continuously rams even at close range\n",
    "    \n",
    "    Speed scaling:\n",
    "    - > 1000mm: 0.4 speed (chasing)\n",
    "    - > 600mm: 0.6 speed (rushing)\n",
    "    - > 300mm: 0.8 speed (engaging)\n",
    "    - <= 300mm: 1.0 speed (destroy - keep ramming)\n",
    "    \"\"\"\n",
    "    global dog_movement\n",
    "    \n",
    "    if human_detected and distance != float('inf'):\n",
    "        # Draw orange box\n",
    "        cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), \n",
    "                     (int(bbox[2]), int(bbox[3])), (0, 165, 255), 2)\n",
    "        \n",
    "        bbox_center_x = (bbox[0] + bbox[2]) / 2\n",
    "        frame_center_x = camera.width / 2\n",
    "\n",
    "        # dog movement logic\n",
    "        if 250 < distance <= 400:\n",
    "            if not dog_movement['active']:\n",
    "                dog_movement['active'] = True\n",
    "                dog_movement['barking'] = 0\n",
    "                dog_movement['charging'] = 'forward'\n",
    "                dog_movement['rush'] = time.time()\n",
    "\n",
    "                # 3 to laps of attacking\n",
    "            if dog_movement['barking'] < 1:\n",
    "                current_time = time.time()\n",
    "                stopped = current_time - dog_movement['rush']\n",
    "\n",
    "                    #go to attack forward like warning in a way\n",
    "                if dog_movement['charging'] == 'forward':\n",
    "                    if stopped < 0.5:\n",
    "                        robot.forward(0.8)\n",
    "                        cv2.putText(depth_colormap, \n",
    "                                        f'dog or bull: action {dog_movement[\"barking\"]+1}/1 - {distance:.0f}mm',\n",
    "                                        (80, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)\n",
    "                        print(f'dog or bull : forward movement {dog_movement[\"barking\"]+1}/1', end='\\r')\n",
    "                    else: # will be go backward\n",
    "                        dog_movement['charging'] = 'backward'\n",
    "                        dog_movement['rush'] = current_time\n",
    "                    #go back breiftly\n",
    "                elif dog_movement['charging'] == 'backward':\n",
    "                    if stopped < 0.5:\n",
    "                            robot.backward(0.8)\n",
    "                            cv2.putText(depth_colormap, \n",
    "                                        f'dog or bull: go back {dog_movement[\"barking\"]+1}/1- {distance:.0f}mm',\n",
    "                                        (80, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)\n",
    "                            print(f'dog or bull : back {dog_movement[\"barking\"]+1}/1', end='\\r')\n",
    "                    else: # one cycle\n",
    "                            dog_movement['barking']  += 1\n",
    "                            dog_movement['charging'] = 'forward'\n",
    "                            dog_movement['rush'] = current_time\n",
    "\n",
    "                return\n",
    "                \n",
    "            else: # finsh the 1 cycle\n",
    "                    dog_movement['active'] = False   \n",
    "\n",
    "        if distance > 400 or distance <= 250:\n",
    "            dog_movement['active'] = False\n",
    "            dog_movement['barking'] = 0\n",
    "\n",
    "        # Determine speed based on distance - faster when farther\n",
    "        if distance > 1000:\n",
    "            speed = 0.4\n",
    "            intensity = \"chasing\"\n",
    "            color = (255, 165, 0)  # orange\n",
    "        elif distance > 600:\n",
    "            speed = 0.6\n",
    "            intensity = \"chasing\"\n",
    "            color = (255, 69, 0)  # red-orange\n",
    "        elif distance > 400:\n",
    "            speed = 0.8\n",
    "            intensity = \"engage attack\"\n",
    "            color = (255, 0, 0)  # red\n",
    "        elif distance > 250:\n",
    "            speed = 0.7\n",
    "            intensity = \"attack\"\n",
    "            color = (255, 0, 0)  # red\n",
    "        else:\n",
    "            # Close range - KEEP PUSHING at maximum speed\n",
    "            speed = 1.0\n",
    "            intensity = \"destroy\"\n",
    "            color = (139, 0, 0)  # dark-red\n",
    "        \n",
    "        \n",
    "        # Always move forward or turn toward human - NEVER STOP\n",
    "        if bbox_center_x < frame_center_x - 50:\n",
    "            robot.left(0.5)\n",
    "            cv2.putText(depth_colormap, f'AGGRESSIVE: {intensity} LEFT {distance:.0f}mm', \n",
    "                       (100, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        elif bbox_center_x > frame_center_x + 50:\n",
    "            robot.right(0.5)\n",
    "            cv2.putText(depth_colormap, f'AGGRESSIVE: {intensity} RIGHT {distance:.0f}mm', \n",
    "                       (100, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        else:\n",
    "            robot.forward(speed)\n",
    "            cv2.putText(depth_colormap, f'AGGRESSIVE: {intensity} {distance:.0f}mm!', \n",
    "                       (120, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        \n",
    "        print(f'Aggression: {intensity} {distance:.0f}mm (speed: {speed})', end='\\r')\n",
    "    else:\n",
    "\n",
    "        dog_movement['active'] = False\n",
    "        dog_movement['barking'] = 0\n",
    "        # No human detected - spin to search\n",
    "        robot.spinRight(0.3)\n",
    "        cv2.putText(depth_colormap, 'AGGRESSIVE: Searching for target...', \n",
    "                   (130, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "        print('Aggression: Checking for target', end='\\r')\n",
    "\n",
    "def curious_behavior(frame, depth_colormap, human_detected, distance, bbox):\n",
    "    \"\"\"\n",
    "    Curious behavior - cautious investigation with circling\n",
    "    \n",
    "    Behavior pattern:\n",
    "    - Far (>800mm): Approach with tracking\n",
    "    - Medium (400-800mm): Slow approach\n",
    "    - Close (<400mm): Circle around to investigate\n",
    "    - None detected: Explore environment\n",
    "    \n",
    "    Circling achieved by combining forward movement with rotation\n",
    "    \"\"\"\n",
    "\n",
    "    if human_detected and distance != float('inf'):\n",
    "        # Draw cyan curiosity indicator\n",
    "        cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), \n",
    "                     (int(bbox[2]), int(bbox[3])), (255, 255, 0), 2)\n",
    "        \n",
    "        # Calculate position for tracking\n",
    "        bbox_center_x = (bbox[0] + bbox[2]) / 2\n",
    "        frame_center_x = camera.width / 2\n",
    "        \n",
    "        if distance < 400:\n",
    "            # Close range - circle around to investigate\n",
    "            robot.forward(0.2)  # Move forward slowly\n",
    "            robot.right(0.3)    # While turning\n",
    "            cv2.putText(depth_colormap, f'CURIOUS: Investigating {distance:.0f}mm', \n",
    "                       (120, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "        elif distance < 800:\n",
    "            if bbox_center_x < frame_center_x - 50:\n",
    "                robot.left(0.2)\n",
    "            elif bbox_center_x > frame_center_x + 50:\n",
    "                robot.right(0.2)\n",
    "            else:\n",
    "                robot.forward(0.2)\n",
    "            cv2.putText(depth_colormap, f'CURIOUS: Approaching {distance:.0f}mm', \n",
    "                       (120, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 100), 2)\n",
    "        else:\n",
    "            if bbox_center_x < frame_center_x - 50:\n",
    "                robot.left(0.3)\n",
    "            elif bbox_center_x > frame_center_x + 50:\n",
    "                robot.right(0.3)\n",
    "            else:\n",
    "                robot.forward(0.4)\n",
    "            cv2.putText(depth_colormap, f'CURIOUS: Interested {distance:.0f}mm', \n",
    "                       (140, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 200, 0), 2)\n",
    "    else:\n",
    "        # No human detected - explore environment\n",
    "        robot.forward(0.2)\n",
    "        robot.right(0.2)\n",
    "        cv2.putText(depth_colormap, 'CURIOUS: Exploring...', \n",
    "                   (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ae896",
   "metadata": {},
   "source": [
    "**Step 5: Keyboard Control and Main Callback**\n",
    "\n",
    "Based on Answer_keyboard_control.ipynb pattern for keyboard input handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b2f8138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gesture recognition initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1764928169.695426    9530 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1764928169.699366    9530 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n",
      "Error in cpuinfo: prctl(PR_SVE_GET_VL) failed\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1764928169.740418    9650 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764928169.788518    9651 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764928169.790896    9651 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764928169.791192    9651 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# NEW: Gesture Recognition Handler\n",
    "class GestureModeHandler:\n",
    "    \"\"\"Handles MediaPipe gesture recognition and mode switching\"\"\"\n",
    "    def __init__(self):\n",
    "        self.current_mode = 'stop'\n",
    "        self.last_gesture_time = 0\n",
    "        self.gesture_cooldown = 1.0  # 1 second between gestures\n",
    "    \n",
    "    def print_result(self, result, output_image, timestamp_ms):\n",
    "        global current_behavior\n",
    "        \n",
    "        # Cooldown to prevent rapid switching\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_gesture_time < self.gesture_cooldown:\n",
    "            return\n",
    "        \n",
    "        if result.gestures:\n",
    "            top_gesture = result.gestures[0][0]\n",
    "            confidence = top_gesture.score\n",
    "            name = top_gesture.category_name\n",
    "            \n",
    "            # Debug: Show detected gestures\n",
    "            print(f\"Detected: {name} (confidence: {confidence:.2f})\")\n",
    "            \n",
    "            if confidence >= 0.7:  # High confidence threshold\n",
    "                old_mode = current_behavior\n",
    "                \n",
    "                # Map gestures to behaviors\n",
    "                if name == \"Thumb_Up\":\n",
    "                    current_behavior = 'love'\n",
    "                elif name == \"Thumb_Down\":\n",
    "                    current_behavior = 'fear'\n",
    "                elif name == \"Open_Palm\":\n",
    "                    current_behavior = 'curious'\n",
    "                elif name in [\"Victory\"]:\n",
    "                    current_behavior = 'aggressive'\n",
    "                #elif name == \"Victory\":\n",
    "                    #current_behavior = 'stop'\n",
    "                    robot.stop()\n",
    "                \n",
    "                # Only print when mode changes\n",
    "                if old_mode != current_behavior:\n",
    "                    self.last_gesture_time = current_time\n",
    "                    print(f'\\n[GESTURE] {name} â†’ {current_behavior.upper()} mode')\n",
    "\n",
    "# Initialize gesture handler\n",
    "gesture_handler = GestureModeHandler()\n",
    "\n",
    "# Create gesture recognizer with model\n",
    "gesture_options = GestureRecognizerOptions(\n",
    "    base_options=BaseOptions(model_asset_path='gesture_recognizer.task'),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "    result_callback=gesture_handler.print_result\n",
    ")\n",
    "\n",
    "# Initialize recognizer\n",
    "gesture_recognizer = GestureRecognizer.create_from_options(gesture_options)\n",
    "\n",
    "print(\"âœ… Gesture recognition initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8755269c-7190-4bff-8868-c5a6813ed0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Gesture-Controlled Robot Active ===\n",
      "Gesture controls:\n",
      "  ðŸ‘ Thumb Up = Love\n",
      "  ðŸ‘Ž Thumb Down = Fear\n",
      "  âœ‹ Open Palm = Curious\n",
      "  âœŠ Victory = Aggressive\n",
      "======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1764928170.034239    9650 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "  WARNING: The scripts f2py, f2py3 and f2py3.10 are installed in '/home/robotics/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
      "jaxlib 0.6.2 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
      "jax 0.6.2 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
      "geopandas 1.1.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/05/2025-09:49:40] [TRT] [I] Loaded engine size: 52 MiB\n",
      "[12/05/2025-09:49:40] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "[12/05/2025-09:49:40] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +36, now: CPU 1, GPU 84 (MiB)\n",
      "Detected: Thumb_Up (confidence: 0.64)\n",
      "Detected: Thumb_Up (confidence: 0.70)\n",
      "\n",
      "[GESTURE] Thumb_Up â†’ LOVE mode\n",
      "Detected: None (confidence: 0.51)\n",
      "Detected: None (confidence: 0.79)\n",
      "Detected: None (confidence: 0.65)\n",
      "Detected: Thumb_Down (confidence: 0.46)\n",
      "Detected: Thumb_Down (confidence: 0.49)\n",
      "Detected: Thumb_Down (confidence: 0.71)\n",
      "\n",
      "[GESTURE] Thumb_Down â†’ FEAR mode\n",
      "Detected: Thumb_Down (confidence: 0.50)\n",
      "Detected: Thumb_Down (confidence: 0.83)\n",
      "Detected: Thumb_Down (confidence: 0.80)\n",
      "Detected: Thumb_Down (confidence: 0.73)\n",
      "Detected: None (confidence: 0.61)\n",
      "Detected: None (confidence: 0.50)\n",
      "Detected: None (confidence: 0.60)\n",
      "Detected: None (confidence: 0.52)\n",
      "Detected: Open_Palm (confidence: 0.53)\n",
      "Detected: Open_Palm (confidence: 0.56)\n",
      "Detected: Open_Palm (confidence: 0.52)\n",
      "Detected: None (confidence: 0.50)\n",
      "Detected: Open_Palm (confidence: 0.67)\n",
      "Detected: Open_Palm (confidence: 0.64)\n",
      "Detected: Open_Palm (confidence: 0.67)\n",
      "Detected: Open_Palm (confidence: 0.69)\n",
      "Detected: Open_Palm (confidence: 0.67)\n",
      "Detected: Open_Palm (confidence: 0.68)\n",
      "Detected: Open_Palm (confidence: 0.70)\n",
      "\n",
      "[GESTURE] Open_Palm â†’ CURIOUS mode\n",
      "Detected: Open_Palm (confidence: 0.58)\n",
      "Detected: None (confidence: 0.55)\n",
      "Detected: None (confidence: 0.98)\n",
      "Detected: None (confidence: 0.71)\n",
      "Detected: Victory (confidence: 0.83)\n",
      "\n",
      "[GESTURE] Victory â†’ AGGRESSIVE mode\n",
      "Detected: None (confidence: 0.80)\n",
      "Detected: None (confidence: 0.55)\n",
      "Detected: None (confidence: 0.82)\n",
      "Detected: None (confidence: 0.83)\n",
      "Detected: None (confidence: 0.80)\n",
      "Detected: None (confidence: 0.82)\n",
      "Detected: None (confidence: 0.86)\n",
      "Detected: None (confidence: 0.82)\n",
      "Detected: None (confidence: 0.85)\n",
      "Detected: None (confidence: 0.84)\n",
      "Detected: None (confidence: 0.87)\n",
      "Detected: None (confidence: 0.88)\n",
      "Detected: None (confidence: 0.91)\n",
      "Detected: None (confidence: 0.90)\n",
      "Detected: None (confidence: 0.91)\n",
      "Detected: None (confidence: 0.87)\n"
     ]
    }
   ],
   "source": [
    "def main_callback(change):\n",
    "    \"\"\"\n",
    "    Main callback function with gesture recognition\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get updated frame\n",
    "        frame = change['new']\n",
    "    \n",
    "        if frame is None:\n",
    "            return\n",
    "\n",
    "        # Create depth colormap for display\n",
    "        depth_colormap = cv2.applyColorMap(\n",
    "            cv2.convertScaleAbs(camera.depth_image, alpha=0.03), \n",
    "            cv2.COLORMAP_JET)\n",
    "        \n",
    "        # NEW: Process gesture recognition\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "        timestamp_ms = int(time.time() * 1000)\n",
    "        gesture_recognizer.recognize_async(mp_image, timestamp_ms)\n",
    "    \n",
    "        # Detect humans and measure distance\n",
    "        human_detected, distance, bbox = detect_human(frame)\n",
    "    \n",
    "        # Execute behavior based on current mode\n",
    "        if current_behavior == 'love':\n",
    "            love_behavior(frame, depth_colormap, human_detected, distance, bbox)\n",
    "        elif current_behavior == 'fear':\n",
    "            fear_behavior(frame, depth_colormap, human_detected, distance, bbox)\n",
    "        elif current_behavior == 'aggressive':\n",
    "            aggressive_behavior(frame, depth_colormap, human_detected, distance, bbox)\n",
    "        elif current_behavior == 'curious':\n",
    "            curious_behavior(frame, depth_colormap, human_detected, distance, bbox)\n",
    "        else:  # stop\n",
    "            robot.stop()\n",
    "            cv2.putText(depth_colormap, 'STOPPED', \n",
    "                   (250, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 2)\n",
    "    \n",
    "        # Display current behavior mode on frame\n",
    "        cv2.putText(frame, f'Mode: {current_behavior.upper()}', \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    \n",
    "        # Display images\n",
    "        scale = 0.3\n",
    "        resized_color = cv2.resize(frame, None, fx=scale, fy=scale, \n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "        resized_depth = cv2.resize(depth_colormap, None, fx=scale, fy=scale, \n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "        display_color.value = bgr8_to_jpeg(resized_color)\n",
    "        display_depth.value = bgr8_to_jpeg(resized_depth)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Main callback error: {e}\")\n",
    "\n",
    "camera.observe(main_callback, names=['color_value'])\n",
    "\n",
    "print('\\n=== Gesture-Controlled Robot Active ===')\n",
    "print('Gesture controls:')\n",
    "print('  ðŸ‘ Thumb Up = Love')\n",
    "print('  ðŸ‘Ž Thumb Down = Fear')\n",
    "print('  âœ‹ Open Palm = Curious')\n",
    "print('  âœŒï¸ Victory = Aggressive')\n",
    "#print('  âœŒï¸ Victory/Peace = Stop')\n",
    "print('======================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc2e85",
   "metadata": {},
   "source": [
    "**To stop the system:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "725c0cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System stopped - camera and robot halted\n"
     ]
    }
   ],
   "source": [
    "# stop camera thread and motors\n",
    "camera.stop()\n",
    "print('System stopped - camera and robot halted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959adf08",
   "metadata": {},
   "source": [
    "## Implementation Notes\n",
    "\n",
    "### Distance Calculation\n",
    "Distance is measured by:\n",
    "1. Getting bounding box from YOLO: `bbox = result.boxes.xyxy[i]` (format: [x1, y1, x2, y2])\n",
    "2. Calculating center: `center_x = (bbox[0] + bbox[2]) / 2`, `center_y = (bbox[1] + bbox[3]) / 2`\n",
    "3. Reading depth at center: `distance = camera.depth_image[center_y, center_x]`\n",
    "4. Depth image returns distance in millimeters\n",
    "\n",
    "\n",
    "### Behavior Distance Thresholds:\n",
    "\n",
    "**Love:** 450mm (too close) | 450-600mm (perfect) | 600-1200mm (medium) | >1200mm (far)\n",
    "\n",
    "**Fear:** 500mm (danger) | 900mm (caution) | 1300mm (alert) | >1300mm (safe)\n",
    "\n",
    "**Aggressive:** 300mm (ram) | 600mm (engage) | 1000mm (rush) | >1000mm (chase)\n",
    "\n",
    "**Curious:** 400mm (investigate) | 800mm (approach) | >800mm (interested)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e457b8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
