{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Person Following with Obstacle Avoidance\n",
    "\n",
    "This notebook implements intelligent person following that:\n",
    "- Locks onto and tracks a specific person\n",
    "- Avoids obstacles (other people and objects) using YOLO detection\n",
    "- Maintains safe distance while following\n",
    "- Navigates around obstacles to continue following\n",
    "- Handles occlusions (when target person temporarily disappears)\n",
    "- Has emergency depth-based collision avoidance\n",
    "\n",
    "**Keyboard controls:**\n",
    "- 'g' = Go (start following first detected person)\n",
    "- 's' = Stop (halt robot and reset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Display Widgets\n",
    "\n",
    "Standard widget setup for displaying color and depth images side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "\n",
    "# Create two widgets for displaying images\n",
    "display_color = widgets.Image(format='jpeg', width='45%')\n",
    "display_depth = widgets.Image(format='jpeg', width='45%')\n",
    "layout = widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth], layout=layout)\n",
    "display(sidebyside)\n",
    "\n",
    "# Convert numpy array to jpeg for displaying\n",
    "def bgr8_to_jpeg(value):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load YOLO Model\n",
    "\n",
    "Load the TensorRT YOLO model for detecting humans and objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLO11 TensorRT model\n",
    "model = YOLO(\"yolo11l_half.engine\")\n",
    "print(\"YOLO model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Person Re-identification System\n",
    "\n",
    "This class uses HSV color histograms to track the same person across frames.\n",
    "When a person is first detected, their clothing colors are saved.\n",
    "In future frames, detected people are matched against these stored patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class HistogramPersonIdentifier:\n",
    "    \"\"\"\n",
    "    Person re-identification using HSV histogram matching.\n",
    "    \n",
    "    How it works:\n",
    "    1. Extract clothing region from person's bounding box\n",
    "    2. Convert to HSV color space (more robust than RGB)\n",
    "    3. Calculate histogram of Hue and Saturation channels\n",
    "    4. Compare new detections against stored histograms\n",
    "    5. Assign consistent person_id if match found\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, similarity_thresh=0.4, h_bins=16, s_bins=16):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            similarity_thresh: Correlation score threshold (0-1)\n",
    "                              Lower = more lenient matching\n",
    "            h_bins: Number of Hue histogram bins\n",
    "            s_bins: Number of Saturation histogram bins\n",
    "        \"\"\"\n",
    "        self.person_db = {}  # person_id -> histogram\n",
    "        self.next_id = 0\n",
    "        self.similarity_thresh = similarity_thresh\n",
    "        self.h_bins = h_bins\n",
    "        self.s_bins = s_bins\n",
    "\n",
    "    def _get_histogram(self, roi_bgr):\n",
    "        \"\"\"Compute normalized HSV histogram for a region of interest.\"\"\"\n",
    "        hsv = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2HSV)\n",
    "        # Calculate 2D histogram (Hue and Saturation only, ignore Value)\n",
    "        hist = cv2.calcHist(\n",
    "            [hsv], [0, 1], None,\n",
    "            [self.h_bins, self.s_bins],\n",
    "            [0, 180, 0, 256]  # H range: 0-180, S range: 0-256\n",
    "        )\n",
    "        # Normalize to 0-1 range for comparison\n",
    "        cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)\n",
    "        return hist\n",
    "\n",
    "    def _match_person(self, hist):\n",
    "        \"\"\"Find best matching person ID from database, or None if no good match.\"\"\"\n",
    "        best_id = None\n",
    "        best_score = -1\n",
    "\n",
    "        # Compare against all known people\n",
    "        for pid, ref_hist in self.person_db.items():\n",
    "            # Correlation comparison (higher = more similar)\n",
    "            score = cv2.compareHist(ref_hist, hist, cv2.HISTCMP_CORREL)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_id = pid\n",
    "\n",
    "        # Return ID only if similarity is above threshold\n",
    "        if best_score > self.similarity_thresh:\n",
    "            return best_id\n",
    "        return None\n",
    "\n",
    "    def assign_ids(self, frame, detections):\n",
    "        \"\"\"\n",
    "        Assign stable person IDs to all detected humans in frame.\n",
    "        \n",
    "        Returns:\n",
    "            List of tuples: (x1, y1, x2, y2, person_id)\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        for box in detections.boxes:\n",
    "            cls = int(box.cls[0])\n",
    "            if cls != 0:  # Only process PERSON class (class 0)\n",
    "                continue\n",
    "\n",
    "            # Extract bounding box coordinates\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "            if roi.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Get histogram for this detection\n",
    "            hist = self._get_histogram(roi)\n",
    "            pid = self._match_person(hist)\n",
    "\n",
    "            # Create new identity if no match found\n",
    "            if pid is None:\n",
    "                pid = self.next_id\n",
    "                self.person_db[pid] = hist\n",
    "                self.next_id += 1\n",
    "\n",
    "            results.append((x1, y1, x2, y2, pid))\n",
    "\n",
    "        return results\n",
    "\n",
    "# Initialize the identifier\n",
    "identifier = HistogramPersonIdentifier(similarity_thresh=0.4)\n",
    "print(\"Person identifier initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Camera System Initialization\n",
    "\n",
    "Initialize ZED2i camera with:\n",
    "- VGA resolution (672x376) for faster processing\n",
    "- ULTRA depth mode for accurate distance measurements\n",
    "- Threaded capture for real-time performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "import pyzed.sl as sl\n",
    "import threading\n",
    "import motors\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "# Initialize robot motor control\n",
    "robot = motors.MotorsYukon(mecanum=False)\n",
    "\n",
    "class Camera(SingletonConfigurable):\n",
    "    \"\"\"\n",
    "    Camera class with traitlets for real-time updates.\n",
    "    Captures color and depth images in separate thread.\n",
    "    \"\"\"\n",
    "    color_value = traitlets.Any()  # Triggers callback when new frame arrives\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "\n",
    "        # Initialize ZED camera\n",
    "        self.zed = sl.Camera()\n",
    "        init_params = sl.InitParameters()\n",
    "        init_params.camera_resolution = sl.RESOLUTION.VGA  # 672x376\n",
    "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "        init_params.coordinate_units = sl.UNIT.MILLIMETER\n",
    "\n",
    "        # Open camera\n",
    "        status = self.zed.open(init_params)\n",
    "        if status != sl.ERROR_CODE.SUCCESS:\n",
    "            print(\"Camera Open:\", repr(status), \"Exit program.\")\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "        self.thread_runnning_flag = False\n",
    "\n",
    "        # Get camera resolution\n",
    "        camera_info = self.zed.get_camera_information()\n",
    "        self.width = camera_info.camera_configuration.resolution.width\n",
    "        self.height = camera_info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(self.width, self.height, sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(self.width, self.height, sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "        \n",
    "        print(f\"Camera initialized: {self.width}x{self.height}\")\n",
    "\n",
    "    def _capture_frames(self):\n",
    "        \"\"\"Threaded capture loop - runs continuously.\"\"\"\n",
    "        while self.thread_runnning_flag:\n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "                # Retrieve images\n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "                \n",
    "                # Convert BGRA to BGR\n",
    "                self.color_value_BGRA = self.image.get_data()\n",
    "                self.color_value = cv2.cvtColor(self.color_value_BGRA, cv2.COLOR_BGRA2BGR)\n",
    "                # Get depth as numpy array\n",
    "                self.depth_image = np.asanyarray(self.depth.get_data())\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the capture thread.\"\"\"\n",
    "        if not self.thread_runnning_flag:\n",
    "            self.thread_runnning_flag = True\n",
    "            self.thread = threading.Thread(target=self._capture_frames)\n",
    "            self.thread.start()\n",
    "            print(\"Camera thread started\")\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the capture thread and motors.\"\"\"\n",
    "        if self.thread_runnning_flag:\n",
    "            self.thread_runnning_flag = False\n",
    "            self.thread.join()\n",
    "            robot.stop()\n",
    "            print(\"Camera stopped\")\n",
    "\n",
    "# Initialize and start camera\n",
    "camera = Camera()\n",
    "camera.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Detection and Navigation Functions\n",
    "\n",
    "This cell contains all the core logic:\n",
    "1. **detect_target_human()** - Finds and tracks the target person\n",
    "2. **detect_obstacles_yolo()** - Identifies obstacles using YOLO (other people + objects)\n",
    "3. **check_depth_emergency()** - Emergency collision avoidance using depth\n",
    "4. **smart_follow_with_avoidance()** - Main navigation logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "target_person_id = None  # ID of person we're following\n",
    "frames_without_target = 0  # Counter for lost target\n",
    "\n",
    "def detect_target_human(frame):\n",
    "    \"\"\"\n",
    "    Detect and track the target person.\n",
    "    \n",
    "    On first detection: Locks onto first person seen\n",
    "    Subsequent frames: Searches for that specific person_id\n",
    "    \n",
    "    Returns:\n",
    "        (human_detected, distance, bbox, person_id)\n",
    "        - human_detected: bool\n",
    "        - distance: float (mm) or inf\n",
    "        - bbox: [x1, y1, x2, y2] or None\n",
    "        - person_id: int or None\n",
    "    \"\"\"\n",
    "    global target_person_id, identifier\n",
    "    \n",
    "    # Run YOLO detection\n",
    "    results = model(frame, verbose=False)[0]\n",
    "    \n",
    "    # Get person IDs from histogram matching\n",
    "    tracks = identifier.assign_ids(frame, results)\n",
    "    \n",
    "    if len(tracks) == 0:\n",
    "        return False, float('inf'), None, target_person_id\n",
    "    \n",
    "    # LOCK ONTO FIRST PERSON if not already locked\n",
    "    if target_person_id is None:\n",
    "        target_person_id = tracks[0][4]  # Get person_id from first detection\n",
    "        print(f\"ðŸŽ¯ LOCKED onto person ID: {target_person_id}\")\n",
    "    \n",
    "    # FIND TARGET PERSON in current detections\n",
    "    target_found = None\n",
    "    for x1, y1, x2, y2, pid in tracks:\n",
    "        if pid == target_person_id:\n",
    "            target_found = (x1, y1, x2, y2, pid)\n",
    "            break\n",
    "    \n",
    "    # Target not in current frame\n",
    "    if target_found is None:\n",
    "        return False, float('inf'), None, target_person_id\n",
    "    \n",
    "    # Calculate distance to target\n",
    "    x1, y1, x2, y2, pid = target_found\n",
    "    center_x = int((x1 + x2) / 2)\n",
    "    center_y = int((y1 + y2) / 2)\n",
    "    \n",
    "    # Get depth at center of bounding box\n",
    "    distance = camera.depth_image[center_y, center_x]\n",
    "    \n",
    "    # Handle invalid depth readings\n",
    "    if np.isnan(distance) or distance <= 0:\n",
    "        distance = float('inf')\n",
    "    \n",
    "    bbox = [x1, y1, x2, y2]\n",
    "    return True, distance, bbox, target_person_id\n",
    "\n",
    "\n",
    "def detect_obstacles_yolo(frame, target_person_id):\n",
    "    \"\"\"\n",
    "    Detect obstacles in robot's path using YOLO.\n",
    "    \n",
    "    Obstacles include:\n",
    "    - Other humans (not the target)\n",
    "    - Objects (chairs, tables, bottles, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        List of obstacle dicts with keys:\n",
    "        - type: 'other_human' or 'object'\n",
    "        - distance: float (mm)\n",
    "        - bbox: [x1, y1, x2, y2]\n",
    "        - class: int (for objects)\n",
    "        - person_id: int (for other humans)\n",
    "    \"\"\"\n",
    "    if target_person_id is None:\n",
    "        return []\n",
    "    \n",
    "    results = model(frame, verbose=False)[0]\n",
    "    tracks = identifier.assign_ids(frame, results)\n",
    "    \n",
    "    obstacles_in_path = []\n",
    "    frame_center_x = camera.width / 2\n",
    "    path_width = 120  # Width of robot's path in pixels (tune based on robot size)\n",
    "    \n",
    "    # 1. CHECK OTHER HUMANS AS OBSTACLES\n",
    "    for x1, y1, x2, y2, pid in tracks:\n",
    "        if pid == target_person_id:\n",
    "            continue  # Skip our target person\n",
    "        \n",
    "        bbox_center_x = (x1 + x2) / 2\n",
    "        \n",
    "        # Is this person in robot's forward path?\n",
    "        if abs(bbox_center_x - frame_center_x) < path_width:\n",
    "            center_x = int(bbox_center_x)\n",
    "            center_y = int((y1 + y2) / 2)\n",
    "            distance = camera.depth_image[center_y, center_x]\n",
    "            \n",
    "            # Valid detection within reasonable range\n",
    "            if not np.isnan(distance) and 0 < distance < 2000:\n",
    "                obstacles_in_path.append({\n",
    "                    'type': 'other_human',\n",
    "                    'person_id': pid,\n",
    "                    'distance': distance,\n",
    "                    'bbox': [x1, y1, x2, y2]\n",
    "                })\n",
    "    \n",
    "    # 2. CHECK NON-HUMAN OBJECTS AS OBSTACLES\n",
    "    for box in results.boxes:\n",
    "        cls = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        \n",
    "        if cls != 0 and conf > 0.5:  # Not human, confident detection\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            bbox_center_x = (x1 + x2) / 2\n",
    "            \n",
    "            # Is this object in robot's path?\n",
    "            if abs(bbox_center_x - frame_center_x) < path_width:\n",
    "                center_x = int(bbox_center_x)\n",
    "                center_y = int((y1 + y2) / 2)\n",
    "                distance = camera.depth_image[center_y, center_x]\n",
    "                \n",
    "                if not np.isnan(distance) and 0 < distance < 2000:\n",
    "                    obstacles_in_path.append({\n",
    "                        'type': 'object',\n",
    "                        'class': cls,\n",
    "                        'distance': distance,\n",
    "                        'bbox': [x1, y1, x2, y2]\n",
    "                    })\n",
    "    \n",
    "    return obstacles_in_path\n",
    "\n",
    "\n",
    "def check_depth_emergency(depth_image, emergency_threshold=300):\n",
    "    \"\"\"\n",
    "    Emergency collision avoidance using depth image.\n",
    "    Catches obstacles that YOLO might miss (poles, wires, etc.)\n",
    "    \n",
    "    Args:\n",
    "        emergency_threshold: Distance in mm for emergency stop\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'emergency' (bool) and 'distance' (float)\n",
    "    \"\"\"\n",
    "    depth = np.nan_to_num(depth_image.copy(), nan=0).astype(np.float32)\n",
    "    \n",
    "    # Check center region directly in front of robot\n",
    "    h, w = depth.shape\n",
    "    center_region = depth[h//3:2*h//3, w//3:2*w//3]\n",
    "    \n",
    "    # Filter for valid depths in danger zone\n",
    "    valid = center_region[(center_region > 50) & (center_region < emergency_threshold)]\n",
    "    \n",
    "    if valid.size > 0:\n",
    "        return {\n",
    "            'emergency': True,\n",
    "            'distance': float(valid.min())\n",
    "        }\n",
    "    \n",
    "    return {'emergency': False, 'distance': float('inf')}\n",
    "\n",
    "\n",
    "def smart_follow_with_avoidance(frame, depth_colormap, human_detected, \n",
    "                                 human_distance, human_bbox, obstacles):\n",
    "    \"\"\"\n",
    "    Main navigation logic: Follow human while avoiding obstacles.\n",
    "    \n",
    "    Decision hierarchy:\n",
    "    1. If no human detected -> Search (spin slowly)\n",
    "    2. If obstacles blocking path -> Navigate around them TOWARD human\n",
    "    3. If path clear -> Normal following behavior\n",
    "    \n",
    "    Key feature: Uses human's position to decide which way to go around obstacles\n",
    "    \"\"\"\n",
    "    global frames_without_target\n",
    "    \n",
    "    if not human_detected:\n",
    "        # Target lost - search behavior\n",
    "        frames_without_target += 1\n",
    "        robot.spinRight(0.2)  # Slow rotation to search\n",
    "        cv2.putText(depth_colormap, f'SEARCHING... ({frames_without_target} frames)', (120, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "        return\n",
    "    \n",
    "    # Target found - reset counter\n",
    "    frames_without_target = 0\n",
    "    \n",
    "    # Calculate human's position relative to frame center\n",
    "    human_center_x = (human_bbox[0] + human_bbox[2]) / 2\n",
    "    frame_center_x = camera.width / 2\n",
    "    human_offset = human_center_x - frame_center_x\n",
    "    # Negative offset = human on left, Positive = human on right\n",
    "    \n",
    "    # Find obstacles that are blocking direct path to human\n",
    "    blocking_obstacles = []\n",
    "    for obs in obstacles:\n",
    "        obs_center_x = (obs['bbox'][0] + obs['bbox'][2]) / 2\n",
    "        # Is obstacle in center path?\n",
    "        if abs(obs_center_x - frame_center_x) < 100:\n",
    "            blocking_obstacles.append(obs)\n",
    "    \n",
    "    # DECISION LOGIC\n",
    "    if len(blocking_obstacles) > 0:\n",
    "        # OBSTACLE IN PATH - Navigate around it intelligently\n",
    "        closest_obstacle = min(blocking_obstacles, key=lambda x: x['distance'])\n",
    "        \n",
    "        if closest_obstacle['distance'] < 300:\n",
    "            # DANGER ZONE - Emergency backup\n",
    "            robot.backward(0.25)\n",
    "            cv2.putText(depth_colormap, f'EMERGENCY BACKUP {closest_obstacle[\"distance\"]:.0f}mm', (80, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "        \n",
    "        elif closest_obstacle['distance'] < 600:\n",
    "            # AVOIDANCE ZONE - Go around obstacle toward human\n",
    "            \n",
    "            if human_offset < -80:  # Human is significantly LEFT\n",
    "                robot.left(0.3)\n",
    "                cv2.putText(depth_colormap, 'AVOIDING - Going LEFT toward human', (60, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 165, 0), 2)\n",
    "            \n",
    "            elif human_offset > 80:  # Human is significantly RIGHT\n",
    "                robot.right(0.3)\n",
    "                cv2.putText(depth_colormap, 'AVOIDING - Going RIGHT toward human', (60, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 165, 0), 2)\n",
    "            \n",
    "            else:  # Human roughly centered but blocked\n",
    "                # Default: Go left (arbitrary choice)\n",
    "                robot.left(0.3)\n",
    "                cv2.putText(depth_colormap, 'AVOIDING - Navigating around obstacle', (80, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 165, 0), 2)\n",
    "        \n",
    "        else:\n",
    "            # CAUTION ZONE - Slow approach while monitoring\n",
    "            robot.forward(0.2)\n",
    "            cv2.putText(depth_colormap, f'CAUTIOUS (obstacle at {closest_obstacle[\"distance\"]:.0f}mm)', (80, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 200, 0), 2)\n",
    "    \n",
    "    else:\n",
    "        # NO OBSTACLES - Normal following behavior\n",
    "        # Distance-based speed control\n",
    "        if human_distance < 450:\n",
    "            # Too close - back up\n",
    "            robot.backward(0.2)\n",
    "            cv2.putText(depth_colormap, f'Too close {human_distance:.0f}mm', (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "        \n",
    "        elif human_distance >= 450 and human_distance <= 600:\n",
    "            # Perfect distance - stop\n",
    "            robot.stop()\n",
    "            cv2.putText(depth_colormap, f'Perfect {human_distance:.0f}mm <3', (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)\n",
    "        \n",
    "        elif human_distance > 600 and human_distance <= 1200:\n",
    "            # Medium distance - approach with turning if needed\n",
    "            if abs(human_offset) > 50:\n",
    "                # Human off-center, turn to center them\n",
    "                if human_offset < 0:\n",
    "                    robot.left(0.3)\n",
    "                else:\n",
    "                    robot.right(0.3)\n",
    "            else:\n",
    "                # Human centered, move forward\n",
    "                robot.forward(0.3)\n",
    "            cv2.putText(depth_colormap, f'Approaching {human_distance:.0f}mm', (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 180, 0), 2)\n",
    "        \n",
    "        else:  # distance > 1200\n",
    "            # Far away - approach faster\n",
    "            if abs(human_offset) > 50:\n",
    "                # Turn toward human\n",
    "                if human_offset < 0:\n",
    "                    robot.left(0.4)\n",
    "                else:\n",
    "                    robot.right(0.4)\n",
    "            else:\n",
    "                # Move forward quickly\n",
    "                robot.forward(0.5)\n",
    "            cv2.putText(depth_colormap, f'Coming to you {human_distance:.0f}mm', (120, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2)\n",
    "\n",
    "print(\"Detection and navigation functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Main Control Loop with Keyboard Input\n",
    "\n",
    "Integrates all components:\n",
    "- Keyboard control (g=go, s=stop)\n",
    "- Target person detection and tracking\n",
    "- Obstacle detection (YOLO + depth)\n",
    "- Smart navigation with visual feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global behavior state\n",
    "current_behavior = 'stop'\n",
    "\n",
    "# Create keyboard input widget\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type: g=Go, s=Stop',\n",
    "    description='Control:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "def on_text_change(change):\n",
    "    \"\"\"Handle keyboard input for behavior control.\"\"\"\n",
    "    global current_behavior, target_person_id\n",
    "    \n",
    "    input_value = change['new']\n",
    "    \n",
    "    if len(input_value) > 0:\n",
    "        last_char = input_value[-1].lower()\n",
    "        \n",
    "        if last_char == 'g':\n",
    "            current_behavior = 'go'\n",
    "            print('\\nðŸš€ STARTED - Robot will follow first detected person')\n",
    "            \n",
    "        elif last_char == 's':\n",
    "            current_behavior = 'stop'\n",
    "            target_person_id = None  # Reset target\n",
    "            robot.stop()\n",
    "            print('\\nðŸ›‘ STOPPED - Robot halted, target reset')\n",
    "\n",
    "text_input.observe(on_text_change, names='value')\n",
    "display(text_input)\n",
    "\n",
    "\n",
    "def main_callback(change):\n",
    "    \"\"\"\n",
    "    Main control callback - executed every frame.\n",
    "    \n",
    "    Processing pipeline:\n",
    "    1. Get current frame\n",
    "    2. Detect target human (with person ID tracking)\n",
    "    3. Detect obstacles (other humans + objects)\n",
    "    4. Check for emergency collision\n",
    "    5. Execute appropriate behavior\n",
    "    6. Visualize everything\n",
    "    \"\"\"\n",
    "    global current_behavior, target_person_id\n",
    "    \n",
    "    # Get updated frame from camera\n",
    "    frame = change['new']\n",
    "    depth_image = camera.depth_image\n",
    "    \n",
    "    # Create depth colormap for visualization\n",
    "    depth_colormap = cv2.applyColorMap(\n",
    "        cv2.convertScaleAbs(depth_image, alpha=0.03), \n",
    "        cv2.COLORMAP_JET)\n",
    "    \n",
    "    # 1. DETECT TARGET HUMAN\n",
    "    human_detected, human_distance, human_bbox, target_id = detect_target_human(frame)\n",
    "    \n",
    "    # 2. DETECT OBSTACLES (only if we have a target)\n",
    "    obstacles = detect_obstacles_yolo(frame, target_id) if target_id else []\n",
    "    \n",
    "    # 3. EMERGENCY DEPTH CHECK\n",
    "    emergency = check_depth_emergency(depth_image, emergency_threshold=250)\n",
    "    \n",
    "    # 4. VISUALIZE DETECTIONS\n",
    "    \n",
    "    # Draw target human (green box)\n",
    "    if human_detected and human_bbox:\n",
    "        x1, y1, x2, y2 = map(int, human_bbox)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "        cv2.putText(frame, f'TARGET (ID:{target_id}) {human_distance:.0f}mm', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    # Draw obstacles (different colors)\n",
    "    for obs in obstacles:\n",
    "        bbox = obs['bbox']\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        \n",
    "        # Color: Orange for other humans, Cyan for objects\n",
    "        if obs['type'] == 'other_human':\n",
    "            color = (0, 165, 255)  # Orange\n",
    "            label = f\"Person {obs['person_id']} {obs['distance']:.0f}mm\"\n",
    "        else:\n",
    "            color = (255, 255, 0)  # Cyan\n",
    "            label = f\"Object {obs['distance']:.0f}mm\"\n",
    "        \n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(frame, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)\n",
    "    \n",
    "    # 5. BEHAVIOR EXECUTION\n",
    "    \n",
    "    if current_behavior == 'stop':\n",
    "        # Stopped state\n",
    "        robot.stop()\n",
    "        cv2.putText(depth_colormap, 'ðŸ›‘ STOPPED', (220, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (128, 128, 128), 2)\n",
    "        cv2.putText(frame, 'Press G to start following', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "    \n",
    "    elif emergency['emergency']:\n",
    "        # EMERGENCY STOP - highest priority\n",
    "        robot.stop()\n",
    "        cv2.putText(depth_colormap, f'ðŸš¨ EMERGENCY STOP {emergency[\"distance\"]:.0f}mm!', (100, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 3)\n",
    "        cv2.putText(frame, 'EMERGENCY - Object too close!', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    elif current_behavior == 'go':\n",
    "        # ACTIVE FOLLOWING\n",
    "        smart_follow_with_avoidance(frame, depth_colormap, human_detected, human_distance, human_bbox, obstacles)\n",
    "    \n",
    "    # Display current state on frame\n",
    "    status_text = f'Mode: {current_behavior.upper()}'\n",
    "    if target_id is not None:\n",
    "        status_text += f' | Target ID: {target_id}'\n",
    "    if len(obstacles) > 0:\n",
    "        status_text += f' | Obstacles: {len(obstacles)}'\n",
    "    \n",
    "    cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "    \n",
    "    # 6. DISPLAY IMAGES\n",
    "    scale = 0.3  # Scale down for network efficiency\n",
    "    resized_color = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "    resized_depth = cv2.resize(depth_colormap, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "    display_color.value = bgr8_to_jpeg(resized_color)\n",
    "    display_depth.value = bgr8_to_jpeg(resized_depth)\n",
    "\n",
    "\n",
    "# Attach callback to camera updates\n",
    "camera.observe(main_callback, names=['color_value'])\n",
    "\n",
    "print('\\n' + '='*50)\n",
    "print('ðŸ¤– PERSON FOLLOWING SYSTEM READY')\n",
    "print('='*50)\n",
    "print('\\nKeyboard Controls:')\n",
    "print('  g = GO (start following first detected person)')\n",
    "print('  s = STOP (halt robot and reset target)')\n",
    "print('\\nFeatures:')\n",
    "print('  âœ“ Locks onto specific person')\n",
    "print('  âœ“ Avoids other people and objects')\n",
    "print('  âœ“ Navigates around obstacles intelligently')\n",
    "print('  âœ“ Handles occlusions (searches if target lost)')\n",
    "print('  âœ“ Emergency collision avoidance')\n",
    "print('  ðŸŸ¢ Green box = Target person')\n",
    "print('  ðŸŸ  Orange box = Other people (obstacles)')\n",
    "print('  ðŸ”µ Cyan box = Objects (obstacles)')\n",
    "print('  Status messages show current behavior')\n",
    "print('='*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
