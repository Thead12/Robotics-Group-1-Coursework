{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "1ddd419e-12b6-4c1f-9935-d98ba98aa685",
      "cell_type": "code",
      "source": "# Part B: Person Following with Obstacle Avoidance\n# This implementation builds upon Tutorial 5 (YOLO detection), Tutorial 2 (camera setup),\n# and Tutorial 4 (traitlets pattern) to create a robust person-following system.\n\n\"\"\"\nSTEP 1: Display Widgets Setup\nBased on Tutorial 2 Part A - standard widget configuration for side-by-side display\n\"\"\"\nimport ipywidgets.widgets as widgets\nfrom IPython.display import display\nimport cv2\n\n# Create widgets for displaying images (Tutorial 2 pattern)\ndisplay_color = widgets.Image(format='jpeg', width='45%')\ndisplay_depth = widgets.Image(format='jpeg', width='45%')\nlayout = widgets.Layout(width='100%')\n\nsidebyside = widgets.HBox([display_color, display_depth], layout=layout)\ndisplay(sidebyside)\n\n# Convert numpy array to jpeg for displaying (Tutorial 2)\ndef bgr8_to_jpeg(value):\n    return bytes(cv2.imencode('.jpg', value)[1])\n\n\n\"\"\"\nSTEP 2: Load YOLO Model\nBased on Tutorial 5 - using TensorRT engine for real-time human detection\n\"\"\"\nfrom ultralytics import YOLO\n\n# Load the YOLO11 TensorRT model (Tutorial 5)\nmodel = YOLO(\"yolo11l_half.engine\")\n\n\n\"\"\"\nSTEP 3: Person Re-Identification System\nUses HSV histogram matching to track the same person across frames,\neven when temporarily occluded by other people (handles occlusion requirement)\n\"\"\"\nimport numpy as np\n\nclass PersonTracker:\n    \"\"\"\n    Tracks a specific person using HSV histogram matching.\n    This handles the \"occlusion by other people\" requirement.\n    \n    How it works:\n    1. When target is locked, store their HSV histogram\n    2. For each detection, compare histogram similarity\n    3. Best match above threshold is considered the same person\n    4. This allows re-identification after temporary occlusion\n    \"\"\"\n    \n    def __init__(self, similarity_threshold=0.6, h_bins=16, s_bins=16):\n        self.target_histogram = None  # Stores target person's appearance\n        self.target_locked = False\n        self.similarity_threshold = similarity_threshold\n        self.h_bins = h_bins\n        self.s_bins = s_bins\n        self.frames_lost = 0  # Track how long target has been lost\n        self.max_lost_frames = 30  # Give up after 30 frames (~1 second at 30fps)\n    \n    def compute_histogram(self, roi_bgr):\n        \"\"\"\n        Compute normalized HSV histogram for a region of interest.\n        HSV is better than RGB for tracking because it's more robust to lighting changes.\n        \"\"\"\n        if roi_bgr.size == 0:\n            return None\n        \n        # Convert to HSV color space (more robust to lighting)\n        hsv = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2HSV)\n        \n        # Calculate histogram using Hue and Saturation channels\n        # Ranges: H=[0,180], S=[0,256]\n        hist = cv2.calcHist(\n            [hsv], [0, 1], None,\n            [self.h_bins, self.s_bins],\n            [0, 180, 0, 256]\n        )\n        \n        # Normalize histogram to [0, 1] range\n        cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)\n        return hist\n    \n    def lock_target(self, frame, bbox):\n        \"\"\"\n        Lock onto a specific person by storing their histogram.\n        Called when we first select our target person.\n        \"\"\"\n        x1, y1, x2, y2 = map(int, bbox)\n        roi = frame[y1:y2, x1:x2]\n        \n        self.target_histogram = self.compute_histogram(roi)\n        if self.target_histogram is not None:\n            self.target_locked = True\n            self.frames_lost = 0\n            return True\n        return False\n    \n    def find_target(self, frame, detections):\n        \"\"\"\n        Find the target person among all detected people.\n        Returns (found, bbox, distance) tuple.\n        \n        Uses histogram matching to identify the same person even after occlusion.\n        \"\"\"\n        if not self.target_locked:\n            return False, None, float('inf')\n        \n        best_match_score = -1\n        best_bbox = None\n        best_distance = float('inf')\n        \n        # Compare each detected person to our target histogram\n        for detection in detections:\n            bbox, distance = detection\n            x1, y1, x2, y2 = map(int, bbox)\n            roi = frame[y1:y2, x1:x2]\n            \n            hist = self.compute_histogram(roi)\n            if hist is None:\n                continue\n            \n            # Calculate histogram similarity using correlation\n            # Returns value in [-1, 1], where 1 = perfect match\n            similarity = cv2.compareHist(\n                self.target_histogram, \n                hist, \n                cv2.HISTCMP_CORREL\n            )\n            \n            if similarity > best_match_score:\n                best_match_score = similarity\n                best_bbox = bbox\n                best_distance = distance\n        \n        # Check if best match is good enough\n        if best_match_score > self.similarity_threshold:\n            self.frames_lost = 0\n            return True, best_bbox, best_distance\n        else:\n            # Target not found in this frame\n            self.frames_lost += 1\n            if self.frames_lost > self.max_lost_frames:\n                # Lost target for too long, unlock\n                self.target_locked = False\n            return False, None, float('inf')\n    \n    def reset(self):\n        \"\"\"Reset tracker to allow locking onto a new person.\"\"\"\n        self.target_histogram = None\n        self.target_locked = False\n        self.frames_lost = 0\n\n\n\"\"\"\nSTEP 4: Human Detection and Distance Calculation\nBased on Tutorial 5 - YOLO detection with depth measurement\n\"\"\"\n\ndef detect_all_humans(frame, depth_image, conf_threshold=0.5):\n    \"\"\"\n    Detect all humans in frame and calculate their distances.\n    Returns list of (bbox, distance) tuples.\n    \n    Based on Tutorial 5:\n    - YOLO detects humans (class 0)\n    - For each detection, get bounding box\n    - Calculate center point\n    - Read depth at center to get distance\n    \"\"\"\n    results = model(frame, verbose=False)\n    \n    detections = []\n    \n    for result in results:\n        for i in range(len(result.boxes.cls)):\n            # Only process human detections (class 0)\n            if result.boxes.cls[i] == 0:\n                if result.boxes.conf[i] > conf_threshold:\n                    # Get bounding box coordinates [x1, y1, x2, y2]\n                    bbox = result.boxes.xyxy[i].cpu().numpy()\n                    \n                    # Calculate center of bounding box\n                    center_x = int((bbox[0] + bbox[2]) / 2)\n                    center_y = int((bbox[1] + bbox[3]) / 2)\n                    \n                    # Get distance from depth image at center point\n                    # Tutorial 2 explains depth image usage\n                    if (0 <= center_y < depth_image.shape[0] and \n                        0 <= center_x < depth_image.shape[1]):\n                        distance = depth_image[center_y, center_x]\n                        \n                        # Filter out invalid depth readings\n                        if not np.isnan(distance) and distance > 0:\n                            detections.append((bbox, distance))\n    \n    return detections\n\n\n\"\"\"\nSTEP 5: Obstacle Detection Using YOLO\nDetects objects in central region that could block path to target.\nUses YOLO for semantic understanding - distinguishes people from objects.\n\"\"\"\n\ndef detect_obstacles_semantic(frame, depth_image, target_distance, conf_threshold=0.5):\n    \"\"\"\n    Detect obstacles (people and objects) that are closer than target person.\n    This implements semantic obstacle detection - we know WHAT the obstacle is.\n    \n    Returns (obstacle_detected, obstacle_side, closest_distance)\n    - obstacle_side: 'left', 'right', or 'center'\n    \"\"\"\n    results = model(frame, verbose=False)\n    \n    frame_center_x = frame.shape[1] / 2\n    frame_height = frame.shape[0]\n    frame_width = frame.shape[1]\n    \n    # Define central region where obstacles matter (similar to Tutorial 3 Part B)\n    # We only care about obstacles in the robot's path\n    left_boundary = frame_width * 0.25   # 25% from left\n    right_boundary = frame_width * 0.75  # 75% from left\n    top_boundary = frame_height * 0.25   # 25% from top\n    \n    closest_obstacle_distance = float('inf')\n    obstacle_side = 'center'\n    obstacle_detected = False\n    \n    for result in results:\n        for i in range(len(result.boxes.cls)):\n            # Process all objects (people are class 0, other objects are other classes)\n            if result.boxes.conf[i] > conf_threshold:\n                bbox = result.boxes.xyxy[i].cpu().numpy()\n                \n                # Calculate center and check if in central region\n                center_x = int((bbox[0] + bbox[2]) / 2)\n                center_y = int((bbox[1] + bbox[3]) / 2)\n                \n                # Only consider objects in our path (central region and below top boundary)\n                if (left_boundary <= center_x <= right_boundary and \n                    center_y > top_boundary):\n                    \n                    # Get distance\n                    if (0 <= center_y < depth_image.shape[0] and \n                        0 <= center_x < depth_image.shape[1]):\n                        distance = depth_image[center_y, center_x]\n                        \n                        # Check if obstacle is closer than target\n                        # And closer than our minimum safe distance\n                        if (not np.isnan(distance) and \n                            distance > 0 and \n                            distance < target_distance and \n                            distance < 800):  # 800mm minimum clearance\n                            \n                            if distance < closest_obstacle_distance:\n                                closest_obstacle_distance = distance\n                                obstacle_detected = True\n                                \n                                # Determine which side the obstacle is on\n                                if center_x < frame_center_x - 50:\n                                    obstacle_side = 'left'\n                                elif center_x > frame_center_x + 50:\n                                    obstacle_side = 'right'\n                                else:\n                                    obstacle_side = 'center'\n    \n    return obstacle_detected, obstacle_side, closest_obstacle_distance\n\n\n\"\"\"\nSTEP 6: Camera System Initialization\nBased on Tutorial 2 Part B and Tutorial 4 Part A - threaded camera capture with traitlets\n\"\"\"\nimport traitlets\nimport pyzed.sl as sl\nimport threading\nimport motors\nfrom traitlets.config.configurable import SingletonConfigurable\n\n# Initialize robot motor control (Tutorial 1)\nrobot = motors.MotorsYukon(mecanum=False)\n\nclass Camera(SingletonConfigurable):\n    \"\"\"\n    Camera class using traitlets for real-time frame monitoring.\n    Based on Tutorial 4 Part A pattern - separates data capture from processing.\n    \"\"\"\n    color_value = traitlets.Any()  # Monitored variable for frame updates\n    \n    def __init__(self):\n        super(Camera, self).__init__()\n        \n        # Initialize ZED camera (Tutorial 2 Part B)\n        self.zed = sl.Camera()\n        init_params = sl.InitParameters()\n        init_params.camera_resolution = sl.RESOLUTION.VGA  # 672x376\n        init_params.depth_mode = sl.DEPTH_MODE.ULTRA      # Best depth quality\n        init_params.coordinate_units = sl.UNIT.MILLIMETER  # Distance in mm\n        \n        # Open camera\n        status = self.zed.open(init_params)\n        if status != sl.ERROR_CODE.SUCCESS:\n            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n            self.zed.close()\n            exit(1)\n        \n        self.runtime = sl.RuntimeParameters()\n        self.thread_runnning_flag = False\n        \n        # Get camera resolution\n        camera_info = self.zed.get_camera_information()\n        self.width = camera_info.camera_configuration.resolution.width\n        self.height = camera_info.camera_configuration.resolution.height\n        \n        # Allocate memory for images\n        self.image = sl.Mat(self.width, self.height, sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n        self.depth = sl.Mat(self.width, self.height, sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n    \n    def _capture_frames(self):\n        \"\"\"\n        Threaded frame capture loop (Tutorial 4 Part A pattern).\n        Runs continuously in background, updating images.\n        \"\"\"\n        while self.thread_runnning_flag:\n            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n                # Retrieve color and depth images\n                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n                \n                # Convert BGRA to BGR for OpenCV compatibility\n                self.color_value_BGRA = self.image.get_data()\n                self.color_value = cv2.cvtColor(self.color_value_BGRA, cv2.COLOR_BGRA2BGR)\n                \n                # Get depth as numpy array\n                self.depth_image = np.asanyarray(self.depth.get_data())\n    \n    def start(self):\n        \"\"\"Start the camera capture thread.\"\"\"\n        if not self.thread_runnning_flag:\n            self.thread_runnning_flag = True\n            self.thread = threading.Thread(target=self._capture_frames)\n            self.thread.start()\n    \n    def stop(self):\n        \"\"\"Stop the camera capture thread and motors.\"\"\"\n        if self.thread_runnning_flag:\n            self.thread_runnning_flag = False\n            self.thread.join()\n            robot.stop()\n\n# Initialize and start camera\ncamera = Camera()\ncamera.start()\n\n\n\"\"\"\nSTEP 7: Following Behavior with Differential Drive\nUses smooth curved movement with individual wheel speed control.\nBased on the cosine approach from love_detection.ipynb.\n\"\"\"\n\ndef follow_target(frame, target_bbox, target_distance, depth_colormap):\n    \"\"\"\n    Follow the target person while maintaining optimal distance.\n    Uses differential drive for smooth curved paths.\n    \n    Distance thresholds:\n    - < 600mm: Too close, back up\n    - 600-900mm: Optimal distance, stop\n    - 900-1500mm: Medium range, approach slowly\n    - > 1500mm: Far away, approach faster\n    \"\"\"\n    # Calculate target position relative to frame center\n    bbox_center_x = (target_bbox[0] + target_bbox[2]) / 2\n    frame_center_x = camera.width / 2\n    \n    # Calculate angle offset (normalized to -1 to 1)\n    offset_x = bbox_center_x - frame_center_x\n    max_offset = camera.width / 2\n    normalized_angle = offset_x / max_offset  # -1 (left) to 1 (right)\n    \n    # Convert to radians for differential drive calculation\n    angle_rad = normalized_angle * (np.pi / 4)  # Max ±π/4 for smooth turning\n    \n    # Calculate differential drive speeds using cosine\n    # This creates smooth curved paths instead of discrete turns\n    turning_factor = np.cos(angle_rad)\n    \n    # Draw target bounding box\n    cv2.rectangle(frame, \n                  (int(target_bbox[0]), int(target_bbox[1])), \n                  (int(target_bbox[2]), int(target_bbox[3])), \n                  (0, 255, 0), 3)  # Green box for target\n    \n    # Distance-based behavior\n    if target_distance < 600:\n        # Too close - back up\n        robot.backward(0.2)\n        cv2.putText(depth_colormap, f'TOO CLOSE {target_distance:.0f}mm', \n                   (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n        \n    elif 600 <= target_distance <= 900:\n        # Optimal distance - stop and maintain\n        robot.stop()\n        cv2.putText(depth_colormap, f'OPTIMAL {target_distance:.0f}mm', \n                   (200, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n        \n    elif 900 < target_distance <= 1500:\n        # Medium distance - approach with differential steering\n        base_speed = 0.3\n        \n        if angle_rad > 0.1:  # Target on right\n            left_speed = base_speed\n            right_speed = base_speed * turning_factor\n            robot.frontLeft(speed=left_speed)\n            robot.backLeft(speed=left_speed)\n            robot.frontRight(speed=right_speed)\n            robot.backRight(speed=right_speed)\n            cv2.putText(depth_colormap, f'FOLLOW RIGHT {target_distance:.0f}mm', \n                       (160, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 180, 0), 2)\n            \n        elif angle_rad < -0.1:  # Target on left\n            left_speed = base_speed * turning_factor\n            right_speed = base_speed\n            robot.frontLeft(speed=left_speed)\n            robot.backLeft(speed=left_speed)\n            robot.frontRight(speed=right_speed)\n            robot.backRight(speed=right_speed)\n            cv2.putText(depth_colormap, f'FOLLOW LEFT {target_distance:.0f}mm', \n                       (160, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 180, 0), 2)\n            \n        else:  # Target centered\n            robot.forward(base_speed)\n            cv2.putText(depth_colormap, f'APPROACH {target_distance:.0f}mm', \n                       (200, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 180, 0), 2)\n            \n    else:  # target_distance > 1500\n        # Far away - approach faster\n        base_speed = 0.5\n        \n        if angle_rad > 0.1:  # Target on right\n            left_speed = base_speed\n            right_speed = base_speed * turning_factor\n            robot.frontLeft(speed=left_speed)\n            robot.backLeft(speed=left_speed)\n            robot.frontRight(speed=right_speed)\n            robot.backRight(speed=right_speed)\n            cv2.putText(depth_colormap, f'CHASE RIGHT {target_distance:.0f}mm', \n                       (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2)\n            \n        elif angle_rad < -0.1:  # Target on left\n            left_speed = base_speed * turning_factor\n            right_speed = base_speed\n            robot.frontLeft(speed=left_speed)\n            robot.backLeft(speed=left_speed)\n            robot.frontRight(speed=right_speed)\n            robot.backRight(speed=right_speed)\n            cv2.putText(depth_colormap, f'CHASE LEFT {target_distance:.0f}mm', \n                       (180, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2)\n            \n        else:  # Target centered\n            robot.forward(base_speed)\n            cv2.putText(depth_colormap, f'CHASE {target_distance:.0f}mm', \n                       (220, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2)\n\n\n\"\"\"\nSTEP 8: Obstacle Avoidance Behavior\nWhen obstacle blocks path, navigate around it while trying to maintain\nvisual contact with target person.\n\"\"\"\n\ndef avoid_obstacle(obstacle_side, depth_colormap):\n    \"\"\"\n    Navigate around obstacle based on which side it's on.\n    Simple strategy: turn away from obstacle until path is clear.\n    \n    Args:\n        obstacle_side: 'left', 'right', or 'center'\n    \"\"\"\n    if obstacle_side == 'left':\n        # Obstacle on left, turn right\n        robot.right(0.3)\n        cv2.putText(depth_colormap, 'OBSTACLE LEFT - TURNING RIGHT', \n                   (120, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n        \n    elif obstacle_side == 'right':\n        # Obstacle on right, turn left\n        robot.left(0.3)\n        cv2.putText(depth_colormap, 'OBSTACLE RIGHT - TURNING LEFT', \n                   (120, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n        \n    else:  # center\n        # Obstacle directly ahead, pick a direction (prefer right)\n        robot.right(0.3)\n        cv2.putText(depth_colormap, 'OBSTACLE AHEAD - TURNING RIGHT', \n                   (120, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n\n\n\"\"\"\nSTEP 9: Main Control Loop with Keyboard Input\nBased on Answer_keyboard_control.ipynb pattern\n\"\"\"\n\n# Initialize tracker\ntracker = PersonTracker(similarity_threshold=0.6)\n\n# State machine states\ncurrent_state = 'idle'  # States: 'idle', 'locking', 'following', 'avoiding'\n\n# Create keyboard input widget (Answer_keyboard_control.ipynb pattern)\ntext_input = widgets.Text(\n    value='',\n    placeholder='Type: l=Lock target, s=Stop, r=Reset',\n    description='Control:',\n    disabled=False\n)\n\ndef on_text_change(change):\n    \"\"\"Handle keyboard input commands.\"\"\"\n    global current_state\n    \n    input_value = change['new']\n    \n    if len(input_value) > 0:\n        last_char = input_value[-1].lower()\n        \n        if last_char == 'l':\n            # Lock onto target\n            current_state = 'locking'\n            print('\\nLOCKING - Will lock onto closest person')\n            \n        elif last_char == 's':\n            # Stop everything\n            current_state = 'idle'\n            tracker.reset()\n            robot.stop()\n            print('\\nSTOPPED - Robot halted, tracker reset')\n            \n        elif last_char == 'r':\n            # Reset tracker only\n            tracker.reset()\n            current_state = 'idle'\n            robot.stop()\n            print('\\nRESET - Tracker reset, ready to lock new target')\n\ntext_input.observe(on_text_change, names='value')\ndisplay(text_input)\n\n\ndef main_callback(change):\n    \"\"\"\n    Main control callback - executed every frame when color_value changes.\n    Implements state machine for target locking, following, and obstacle avoidance.\n    \"\"\"\n    global current_state\n    \n    # Get current frame\n    frame = change['new'].copy()\n    depth_image = camera.depth_image\n    \n    # Create depth colormap for visualization\n    depth_colormap = cv2.applyColorMap(\n        cv2.convertScaleAbs(depth_image, alpha=0.03), \n        cv2.COLORMAP_JET\n    )\n    \n    # Detect all humans in frame (Tutorial 5 method)\n    all_detections = detect_all_humans(frame, depth_image)\n    \n    # Draw all detected humans in blue\n    for bbox, distance in all_detections:\n        cv2.rectangle(frame, \n                     (int(bbox[0]), int(bbox[1])), \n                     (int(bbox[2]), int(bbox[3])), \n                     (255, 0, 0), 2)  # Blue boxes for all humans\n    \n    # State machine\n    if current_state == 'idle':\n        robot.stop()\n        cv2.putText(frame, 'IDLE - Press L to lock target', \n                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n        cv2.putText(depth_colormap, 'IDLE', \n                   (270, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 2)\n    \n    elif current_state == 'locking':\n        # Lock onto closest detected person\n        if len(all_detections) > 0:\n            # Find closest person\n            closest_detection = min(all_detections, key=lambda x: x[1])\n            closest_bbox, closest_distance = closest_detection\n            \n            # Lock onto this person\n            if tracker.lock_target(frame, closest_bbox):\n                current_state = 'following'\n                print(f'\\nTARGET LOCKED at {closest_distance:.0f}mm')\n                \n                # Draw green box around locked target\n                cv2.rectangle(frame, \n                            (int(closest_bbox[0]), int(closest_bbox[1])), \n                            (int(closest_bbox[2]), int(closest_bbox[3])), \n                            (0, 255, 0), 3)\n        else:\n            cv2.putText(frame, 'LOCKING - No person detected', \n                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)\n    \n    elif current_state == 'following':\n        # Try to find our target person\n        target_found, target_bbox, target_distance = tracker.find_target(frame, all_detections)\n        \n        if target_found:\n            # Check for obstacles in our path\n            obstacle_detected, obstacle_side, obstacle_distance = detect_obstacles_semantic(\n                frame, depth_image, target_distance\n            )\n            \n            if obstacle_detected:\n                # Obstacle blocking path - switch to avoiding\n                current_state = 'avoiding'\n                cv2.putText(frame, 'AVOIDING OBSTACLE', \n                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n            else:\n                # Path clear - follow target\n                follow_target(frame, target_bbox, target_distance, depth_colormap)\n                cv2.putText(frame, f'FOLLOWING - Distance: {target_distance:.0f}mm', \n                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n        else:\n            # Lost target\n            robot.stop()\n            if tracker.frames_lost > tracker.max_lost_frames:\n                # Lost for too long, give up\n                current_state = 'idle'\n                print('\\nTARGET LOST - Returning to idle')\n            cv2.putText(frame, f'SEARCHING - Target lost for {tracker.frames_lost} frames', \n                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)\n            cv2.putText(depth_colormap, 'TARGET LOST', \n                       (220, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2)\n    \n    elif current_state == 'avoiding':\n        # Try to find target to check if we should resume following\n        target_found, target_bbox, target_distance = tracker.find_target(frame, all_detections)\n        \n        if target_found:\n            # Check if obstacle is still there\n            obstacle_detected, obstacle_side, obstacle_distance = detect_obstacles_semantic(\n                frame, depth_image, target_distance\n            )\n            \n            if obstacle_detected:\n                # Still blocked - keep avoiding\n                avoid_obstacle(obstacle_side, depth_colormap)\n                cv2.putText(frame, 'AVOIDING OBSTACLE', \n                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n            else:\n                # Path clear - resume following\n                current_state = 'following'\n                print('\\nPath clear - Resuming following')\n        else:\n            # Lost target while avoiding\n            robot.stop()\n            current_state = 'idle'\n            print('\\nTarget lost during avoidance - Returning to idle')\n    \n    # Display status information\n    cv2.putText(frame, f'State: {current_state.upper()}', \n               (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n    cv2.putText(frame, f'Tracked: {tracker.target_locked}', \n               (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n    \n    # Display images\n    scale = 0.15\n    resized_color = cv2.resize(frame, None, fx=scale, fy=scale, \n                               interpolation=cv2.INTER_AREA)\n    resized_depth = cv2.resize(depth_colormap, None, fx=scale, fy=scale, \n                               interpolation=cv2.INTER_AREA)\n    display_color.value = bgr8_to_jpeg(resized_color)\n    display_depth.value = bgr8_to_jpeg(resized_depth)\n\n# Attach callback to camera (Tutorial 4 Part A pattern)\ncamera.observe(main_callback, names=['color_value'])\n\nprint('\\n=== Part B: Person Following System ===')\nprint('Commands:')\nprint('  l = Lock onto closest person and start following')\nprint('  s = Stop robot and reset tracker')\nprint('  r = Reset tracker only (keeps running)')\nprint('\\nFeatures:')\nprint('  - Automatic person re-identification after occlusion')\nprint('  - Obstacle avoidance while following')\nprint('  - Smooth curved movement with differential drive')\nprint('  - Maintains optimal distance (600-900mm)')\nprint('========================================')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}