{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part B: Person Following with Obstacle Avoidance\n",
        "\n",
        "This notebook implements intelligent person following that:\n",
        "- Locks onto and tracks a specific person\n",
        "- Avoids obstacles (other people and objects) using YOLO detection\n",
        "- Maintains safe distance while following\n",
        "- Navigates around obstacles to continue following\n",
        "- Handles occlusions (when target person temporarily disappears)\n",
        "- Has emergency depth-based collision avoidance\n",
        "\n",
        "**Keyboard controls:**\n",
        "- 'g' = Go (start following first detected person)\n",
        "- 's' = Stop (halt robot and reset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Display Widgets\n",
        "\n",
        "Standard widget setup for displaying color and depth images side by side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ipywidgets.widgets as widgets\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "\n",
        "# Create two widgets for displaying images\n",
        "display_color = widgets.Image(format='jpeg', width='45%')\n",
        "display_depth = widgets.Image(format='jpeg', width='45%')\n",
        "layout = widgets.Layout(width='100%')\n",
        "\n",
        "sidebyside = widgets.HBox([display_color, display_depth], layout=layout)\n",
        "display(sidebyside)\n",
        "\n",
        "# Convert numpy array to jpeg for displaying\n",
        "def bgr8_to_jpeg(value):\n",
        "    return bytes(cv2.imencode('.jpg', value)[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load YOLO Model\n",
        "\n",
        "Load the TensorRT YOLO model for detecting humans and objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load YOLO11 TensorRT model\n",
        "model = YOLO(\"yolo11l_half.engine\")\n",
        "print(\"YOLO model loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Person Re-identification System\n",
        "\n",
        "This class uses HSV color histograms to track the same person across frames.\n",
        "When a person is first detected, their clothing colors are saved.\n",
        "In future frames, detected people are matched against these stored patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class HistogramPersonIdentifier:\n",
        "    \"\"\"\n",
        "    Person re-identification using HSV histogram matching.\n",
        "    \n",
        "    How it works:\n",
        "    1. Extract clothing region from person's bounding box\n",
        "    2. Convert to HSV color space (more robust than RGB)\n",
        "    3. Calculate histogram of Hue and Saturation channels\n",
        "    4. Compare new detections against stored histograms\n",
        "    5. Assign consistent person_id if match found\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, similarity_thresh=0.4, h_bins=16, s_bins=16):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            similarity_thresh: Correlation score threshold (0-1)\n",
        "                              Lower = more lenient matching\n",
        "            h_bins: Number of Hue histogram bins\n",
        "            s_bins: Number of Saturation histogram bins\n",
        "        \"\"\"\n",
        "        self.person_db = {}  # person_id -> histogram\n",
        "        self.next_id = 0\n",
        "        self.similarity_thresh = similarity_thresh\n",
        "        self.h_bins = h_bins\n",
        "        self.s_bins = s_bins\n",
        "\n",
        "    def _get_histogram(self, roi_bgr):\n",
        "        \"\"\"Compute normalized HSV histogram for a region of interest.\"\"\"\n",
        "        hsv = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2HSV)\n",
        "        # Calculate 2D histogram (Hue and Saturation only, ignore Value)\n",
        "        hist = cv2.calcHist(\n",
        "            [hsv], [0, 1], None,\n",
        "            [self.h_bins, self.s_bins],\n",
        "            [0, 180, 0, 256]  # H range: 0-180, S range: 0-256\n",
        "        )\n",
        "        # Normalize to 0-1 range for comparison\n",
        "        cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)\n",
        "        return hist\n",
        "\n",
        "    def _match_person(self, hist):\n",
        "        \"\"\"Find best matching person ID from database, or None if no good match.\"\"\"\n",
        "        best_id = None\n",
        "        best_score = -1\n",
        "\n",
        "        # Compare against all known people\n",
        "        for pid, ref_hist in self.person_db.items():\n",
        "            # Correlation comparison (higher = more similar)\n",
        "            score = cv2.compareHist(ref_hist, hist, cv2.HISTCMP_CORREL)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_id = pid\n",
        "\n",
        "        # Return ID only if similarity is above threshold\n",
        "        if best_score > self.similarity_thresh:\n",
        "            return best_id\n",
        "        return None\n",
        "\n",
        "    def assign_ids(self, frame, detections):\n",
        "        \"\"\"\n",
        "        Assign stable person IDs to all detected humans in frame.\n",
        "        \n",
        "        Returns:\n",
        "            List of tuples: (x1, y1, x2, y2, person_id)\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        for box in detections.boxes:\n",
        "            cls = int(box.cls[0])\n",
        "            if cls != 0:  # Only process PERSON class (class 0)\n",
        "                continue\n",
        "\n",
        "            # Extract bounding box coordinates\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            roi = frame[y1:y2, x1:x2]\n",
        "\n",
        "            if roi.size == 0:\n",
        "                continue\n",
        "\n",
        "            # Get histogram for this detection\n",
        "            hist = self._get_histogram(roi)\n",
        "            pid = self._match_person(hist)\n",
        "\n",
        "            # Create new identity if no match found\n",
        "            if pid is None:\n",
        "                pid = self.next_id\n",
        "                self.person_db[pid] = hist\n",
        "                self.next_id += 1\n",
        "\n",
        "            results.append((x1, y1, x2, y2, pid))\n",
        "\n",
        "        return results\n",
        "\n",
        "# Initialize the identifier\n",
        "identifier = HistogramPersonIdentifier(similarity_thresh=0.4)\n",
        "print(\"Person identifier initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Camera System Initialization\n",
        "\n",
        "Initialize ZED2i camera with:\n",
        "- VGA resolution (672x376) for faster processing\n",
        "- ULTRA depth mode for accurate distance measurements\n",
        "- Threaded capture for real-time performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import traitlets\n",
        "import pyzed.sl as sl\n",
        "import threading\n",
        "import motors\n",
        "from traitlets.config.configurable import SingletonConfigurable\n",
        "\n",
        "# Initialize robot motor control\n",
        "robot = motors.MotorsYukon(mecanum=False)\n",
        "\n",
        "class Camera(SingletonConfigurable):\n",
        "    \"\"\"\n",
        "    Camera class with traitlets for real-time updates.\n",
        "    Captures color and depth images in separate thread.\n",
        "    \"\"\"\n",
        "    color_value = traitlets.Any()  # Triggers callback when new frame arrives\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Camera, self).__init__()\n",
        "\n",
        "        # Initialize ZED camera\n",
        "        self.zed = sl.Camera()\n",
        "        init_params = sl.InitParameters()\n",
        "        init_params.camera_resolution = sl.RESOLUTION.VGA  # 672x376\n",
        "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
        "        init_params.coordinate_units = sl.UNIT.MILLIMETER\n",
        "\n",
        "        # Open camera\n",
        "        status = self.zed.open(init_params)\n",
        "        if status != sl.ERROR_CODE.SUCCESS:\n",
        "            print(\"Camera Open:\", repr(status), \"Exit program.\")\n",
        "            self.zed.close()\n",
        "            exit(1)\n",
        "\n",
        "        self.runtime = sl.RuntimeParameters()\n",
        "        self.thread_runnning_flag = False\n",
        "\n",
        "        # Get camera resolution\n",
        "        camera_info = self.zed.get_camera_information()\n",
        "        self.width = camera_info.camera_configuration.resolution.width\n",
        "        self.height = camera_info.camera_configuration.resolution.height\n",
        "        self.image = sl.Mat(self.width, self.height, sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
        "        self.depth = sl.Mat(self.width, self.height, sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
        "        \n",
        "        print(f\"Camera initialized: {self.width}x{self.height}\")\n",
        "\n",
        "    def _capture_frames(self):\n",
        "        \"\"\"Threaded capture loop - runs continuously.\"\"\"\n",
        "        while self.thread_runnning_flag:\n",
        "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
        "                # Retrieve images\n",
        "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
        "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
        "                \n",
        "                # Convert BGRA to BGR\n",
        "                self.color_value_BGRA = self.image.get_data()\n",
        "                self.color_value = cv2.cvtColor(self.color_value_BGRA, cv2.COLOR_BGRA2BGR)\n",
        "                # Get depth as numpy array\n",
        "                self.depth_image = np.asanyarray(self.depth.get_data())\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the capture thread.\"\"\"\n",
        "        if not self.thread_runnning_flag:\n",
        "            self.thread_runnning_flag = True\n",
        "            self.thread = threading.Thread(target=self._capture_frames)\n",
        "            self.thread.start()\n",
        "            print(\"Camera thread started\")\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the capture thread and motors.\"\"\"\n",
        "        if self.thread_runnning_flag:\n",
        "            self.thread_runnning_flag = False\n",
        "            self.thread.join()\n",
        "            robot.stop()\n",
        "            print(\"Camera stopped\")\n",
        "\n",
        "# Initialize and start camera\n",
        "camera = Camera()\n",
        "camera.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Detection and Navigation Functions\n",
        "\n",
        "This cell contains all the core logic:\n",
        "1. **detect_target_human()** - Finds and tracks the target person\n",
        "2. **detect_obstacles_yolo()** - Identifies obstacles using YOLO (other people + objects)\n",
        "3. **check_depth_emergency()** - Emergency collision avoidance using depth\n",
        "4. **smart_follow_with_avoidance()** - Main navigation logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global variables\n",
        "target_person_id = None  # ID of person we're following\n",
        "frames_without_target = 0  # Counter for lost target\n",
        "\n",
        "def detect_target_human(frame):\n",
        "    \"\"\"\n",
        "    Detect and track the target person.\n",
        "    \n",
        "    On first detection: Locks onto first person seen\n",
        "    Subsequent frames: Searches for that specific person_id\n",
        "    \n",
        "    Returns:\n",
        "        (human_detected, distance, bbox, person_id)\n",
        "        - human_detected: bool\n",
        "        - distance: float (mm) or inf\n",
        "        - bbox: [x1, y1, x2, y2] or None\n",
        "        - person_id: int or None\n",
        "    \"\"\"\n",
        "    global target_person_id, identifier\n",
        "    \n",
        "    # Run YOLO detection\n",
        "    results = model(frame, verbose=False)[0]\n",
        "    \n",
        "    # Get person IDs from histogram matching\n",
        "    tracks = identifier.assign_ids(frame, results)\n",
        "    \n",
        "    if len(tracks) == 0:\n",
        "        return False, float('inf'), None, target_person_id\n",
        "    \n",
        "    # LOCK ONTO FIRST PERSON if not already locked\n",
        "    if target_person_id is None:\n",
        "        target_person_id = tracks[0][4]  # Get person_id from first detection\n",
        "        print(f\"ðŸŽ¯ LOCKED onto person ID: {target_person_id}\")\n",
        "    \n",
        "    # FIND TARGET PERSON in current detections\n",
        "    target_found = None\n",
        "    for x1, y1, x2, y2, pid in tracks:\n",
        "        if pid == target_person_id:\n",
        "            target_found = (x1, y1, x2, y2, pid)\n",
        "            break\n",
        "    \n",
        "    # Target not in current frame\n",
        "    if target_found is None:\n",
        "        return False, float('inf'), None, target_person_id\n",
        "    \n",
        "    # Calculate distance to target\n",
        "    x1, y1, x2, y2, pid = target_found\n",
        "    center_x = int((x1 + x2) / 2)\n",
        "    center_y = int((y1 + y2) / 2)\n",
        "    \n",
        "    # Get depth at center of bounding box\n",
        "    distance = camera.depth_image[center_y, center_x]\n",
        "    \n",
        "    # Handle invalid depth readings\n",
        "    if np.isnan(distance) or distance <= 0:\n",
        "        distance = float('inf')\n",
        "    \n",
        "    bbox = [x1, y1, x2, y2]\n",
        "    return True, distance, bbox, target_person_id\n",
        "\n",
        "\n",
        "def detect_obstacles_depth(depth_image, human_bbox=None, min_safe_distance=400):\n",
        "    \"\"\"\n",
        "    Detect obstacles using depth image (Tutorial 3 Part B approach).\n",
        "    Excludes the region where target human is located.\n",
        "    \n",
        "    Returns:\n",
        "        dict with 'obstacle_detected' (bool) and 'min_distance' (float)\n",
        "    \"\"\"\n",
        "    depth = np.nan_to_num(depth_image.copy(), nan=0.0).astype(np.float32)\n",
        "    \n",
        "    # Define central area (from Tutorial 3 Part B)\n",
        "    depth[:94, :] = 0      # Top region\n",
        "    depth[282:, :] = 0     # Bottom region  \n",
        "    depth[:, :168] = 0     # Left region\n",
        "    depth[:, 504:] = 0     # Right region\n",
        "    \n",
        "    # Filter depth values (from Tutorial 3 Part B)\n",
        "    depth[depth < 100] = 0    # Too close noise\n",
        "    depth[depth > 1000] = 0   # Too far / unreliable\n",
        "    \n",
        "    # If we have target human bbox, exclude that region from obstacle detection\n",
        "    if human_bbox is not None:\n",
        "        x1, y1, x2, y2 = map(int, human_bbox)\n",
        "        # Add margin around human bbox\n",
        "        margin = 30\n",
        "        x1 = max(0, x1 - margin)\n",
        "        y1 = max(0, y1 - margin)\n",
        "        x2 = min(depth.shape[1], x2 + margin)\n",
        "        y2 = min(depth.shape[0], y2 + margin)\n",
        "        depth[y1:y2, x1:x2] = 0  # Zero out human's region\n",
        "    \n",
        "    # Check if any obstacles remain\n",
        "    if depth.max() == 0:\n",
        "        return {'obstacle_detected': False, 'min_distance': float('inf')}\n",
        "    \n",
        "    # Find minimum distance in remaining valid regions\n",
        "    valid_depths = depth[depth != 0]\n",
        "    min_distance = valid_depths.min()\n",
        "    \n",
        "    # Check if obstacle is within danger zone\n",
        "    if min_distance < min_safe_distance:\n",
        "        return {'obstacle_detected': True, 'min_distance': float(min_distance)}\n",
        "    \n",
        "    return {'obstacle_detected': False, 'min_distance': float(min_distance)}\n",
        "\n",
        "\n",
        "def check_depth_emergency(depth_image, emergency_threshold=300):\n",
        "    \"\"\"\n",
        "    Emergency collision avoidance using depth image.\n",
        "    Catches obstacles that YOLO might miss (poles, wires, etc.)\n",
        "    \n",
        "    Args:\n",
        "        emergency_threshold: Distance in mm for emergency stop\n",
        "    \n",
        "    Returns:\n",
        "        dict with 'emergency' (bool) and 'distance' (float)\n",
        "    \"\"\"\n",
        "    depth = np.nan_to_num(depth_image.copy(), nan=0).astype(np.float32)\n",
        "    \n",
        "    # Check center region directly in front of robot\n",
        "    h, w = depth.shape\n",
        "    center_region = depth[h//3:2*h//3, w//3:2*w//3]\n",
        "    \n",
        "    # Filter for valid depths in danger zone\n",
        "    valid = center_region[(center_region > 50) & (center_region < emergency_threshold)]\n",
        "    \n",
        "    if valid.size > 0:\n",
        "        return {\n",
        "            'emergency': True,\n",
        "            'distance': float(valid.min())\n",
        "        }\n",
        "    \n",
        "    return {'emergency': False, 'distance': float('inf')}\n",
        "\n",
        "\n",
        "def smart_follow_with_avoidance(frame, depth_colormap, human_detected, \n",
        "                                 human_distance, human_bbox, obstacle_info):\n",
        "    \"\"\"\n",
        "    Main navigation logic with simplified depth-based obstacle avoidance.\n",
        "    \"\"\"\n",
        "    global frames_without_target\n",
        "    \n",
        "    if not human_detected:\n",
        "        # Target lost - search behavior\n",
        "        frames_without_target += 1\n",
        "        robot.spinRight(0.2)\n",
        "        cv2.putText(depth_colormap, f'SEARCHING... ({frames_without_target} frames)', \n",
        "                   (120, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
        "        return\n",
        "    \n",
        "    frames_without_target = 0\n",
        "    \n",
        "    # Calculate human's position\n",
        "    human_center_x = (human_bbox[0] + human_bbox[2]) / 2\n",
        "    frame_center_x = camera.width / 2\n",
        "    human_offset = human_center_x - frame_center_x\n",
        "    \n",
        "    # OBSTACLE DETECTED (from Tutorial 3 Part B pattern)\n",
        "    if obstacle_info['obstacle_detected']:\n",
        "        obs_distance = obstacle_info['min_distance']\n",
        "        \n",
        "        if obs_distance < 300:\n",
        "            # Very close - backup and turn away\n",
        "            robot.backward(0.3)\n",
        "            cv2.putText(depth_colormap, f'OBSTACLE! {obs_distance:.0f}mm - BACKING UP', \n",
        "                       (80, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "        \n",
        "        elif obs_distance < 400:\n",
        "            # Close - turn toward human's direction\n",
        "            if abs(human_offset) > 50:\n",
        "                if human_offset < 0:\n",
        "                    robot.left(0.3)\n",
        "                else:\n",
        "                    robot.right(0.3)\n",
        "                cv2.putText(depth_colormap, f'AVOIDING {obs_distance:.0f}mm - TURNING', \n",
        "                           (80, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 165, 0), 2)\n",
        "            else:\n",
        "                # Human centered but blocked - default turn left\n",
        "                robot.left(0.3)\n",
        "                cv2.putText(depth_colormap, f'BLOCKED {obs_distance:.0f}mm - NAVIGATING', \n",
        "                           (80, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 165, 0), 2)\n",
        "        else:\n",
        "            # Moderate distance - slow approach\n",
        "            robot.forward(0.2)\n",
        "            cv2.putText(depth_colormap, f'CAUTIOUS {obs_distance:.0f}mm', \n",
        "                       (120, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 200, 0), 2)\n",
        "    \n",
        "    else:\n",
        "        # NO OBSTACLES - Normal following (same as before)\n",
        "        if human_distance < 450:\n",
        "            robot.backward(0.2)\n",
        "            cv2.putText(depth_colormap, f'Too close {human_distance:.0f}mm', \n",
        "                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "        \n",
        "        elif human_distance >= 450 and human_distance <= 600:\n",
        "            robot.stop()\n",
        "            cv2.putText(depth_colormap, f'Perfect {human_distance:.0f}mm <3', \n",
        "                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)\n",
        "        \n",
        "        elif human_distance > 600 and human_distance <= 1200:\n",
        "            if abs(human_offset) > 50:\n",
        "                if human_offset < 0:\n",
        "                    robot.left(0.3)\n",
        "                else:\n",
        "                    robot.right(0.3)\n",
        "            else:\n",
        "                robot.forward(0.3)\n",
        "            cv2.putText(depth_colormap, f'Approaching {human_distance:.0f}mm', \n",
        "                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 180, 0), 2)\n",
        "        \n",
        "        else:\n",
        "            if abs(human_offset) > 50:\n",
        "                if human_offset < 0:\n",
        "                    robot.left(0.4)\n",
        "                else:\n",
        "                    robot.right(0.4)\n",
        "            else:\n",
        "                robot.forward(0.5)\n",
        "            cv2.putText(depth_colormap, f'Coming to you {human_distance:.0f}mm', \n",
        "                       (120, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2)            cv2.putText(depth_colormap, f'Coming to you {human_distance:.0f}mm', (120, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2)\n",
        "\n",
        "print(\"Detection and navigation functions loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Main Control Loop with Keyboard Input\n",
        "\n",
        "Integrates all components:\n",
        "- Keyboard control (g=go, s=stop)\n",
        "- Target person detection and tracking\n",
        "- Obstacle detection (YOLO + depth)\n",
        "- Smart navigation with visual feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global behavior state\n",
        "current_behavior = 'stop'\n",
        "\n",
        "# Create keyboard input widget\n",
        "text_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Type: g=Go, s=Stop',\n",
        "    description='Control:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "def on_text_change(change):\n",
        "    \"\"\"Handle keyboard input for behavior control.\"\"\"\n",
        "    global current_behavior, target_person_id\n",
        "    \n",
        "    input_value = change['new']\n",
        "    \n",
        "    if len(input_value) > 0:\n",
        "        last_char = input_value[-1].lower()\n",
        "        \n",
        "        if last_char == 'g':\n",
        "            current_behavior = 'go'\n",
        "            print('\\nðŸš€ STARTED - Robot will follow first detected person')\n",
        "            \n",
        "        elif last_char == 's':\n",
        "            current_behavior = 'stop'\n",
        "            target_person_id = None  # Reset target\n",
        "            robot.stop()\n",
        "            print('\\nSTOPPED - Robot halted, target reset')\n",
        "\n",
        "text_input.observe(on_text_change, names='value')\n",
        "display(text_input)\n",
        "\n",
        "\n",
        "def main_callback(change):\n",
        "    \"\"\"\n",
        "    Main control callback - executed every frame.\n",
        "    \n",
        "    Processing pipeline:\n",
        "    1. Get current frame\n",
        "    2. Detect target human (with person ID tracking)\n",
        "    3. Detect obstacles (other humans + objects)\n",
        "    4. Check for emergency collision\n",
        "    5. Execute appropriate behavior\n",
        "    6. Visualize everything\n",
        "    \"\"\"\n",
        "    global current_behavior, target_person_id\n",
        "    \n",
        "    # Get updated frame from camera\n",
        "    frame = change['new']\n",
        "    depth_image = camera.depth_image\n",
        "    \n",
        "    # Create depth colormap for visualization\n",
        "    depth_colormap = cv2.applyColorMap(\n",
        "        cv2.convertScaleAbs(depth_image, alpha=0.03), \n",
        "        cv2.COLORMAP_JET)\n",
        "    \n",
        "    # 1. DETECT TARGET HUMAN\n",
        "    human_detected, human_distance, human_bbox, target_id = detect_target_human(frame)\n",
        "    \n",
        "    # 2. DETECT OBSTACLES (only if we have a target)\n",
        "    obstacle_info = detect_obstacles_depth(depth_image, human_bbox if human_detected else None, min_safe_distance=400)\n",
        "\n",
        "    \n",
        "    # 3. EMERGENCY DEPTH CHECK\n",
        "    emergency = check_depth_emergency(depth_image, emergency_threshold=250)\n",
        "    \n",
        "    # 4. VISUALIZE DETECTIONS\n",
        "    \n",
        "    # Draw target human (green box)\n",
        "    if human_detected and human_bbox:\n",
        "        x1, y1, x2, y2 = map(int, human_bbox)\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
        "        cv2.putText(frame, f'TARGET (ID:{target_id}) {human_distance:.0f}mm', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "    \n",
        "    \n",
        "    # 5. BEHAVIOR EXECUTION\n",
        "    \n",
        "    if current_behavior == 'stop':\n",
        "        # Stopped state\n",
        "        robot.stop()\n",
        "        cv2.putText(depth_colormap, 'ðŸ›‘ STOPPED', (220, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (128, 128, 128), 2)\n",
        "        cv2.putText(frame, 'Press G to start following', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "    \n",
        "    elif emergency['emergency']:\n",
        "        # EMERGENCY STOP - highest priority\n",
        "        robot.stop()\n",
        "        cv2.putText(depth_colormap, f'ðŸš¨ EMERGENCY STOP {emergency[\"distance\"]:.0f}mm!', (100, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 3)\n",
        "        cv2.putText(frame, 'EMERGENCY - Object too close!', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "    \n",
        "    elif current_behavior == 'go':\n",
        "        # ACTIVE FOLLOWING\n",
        "        smart_follow_with_avoidance(frame, depth_colormap, human_detected, human_distance, human_bbox, obstacle_info)\n",
        "    \n",
        "    # Display current state on frame\n",
        "    status_text = f'Mode: {current_behavior.upper()}'\n",
        "    if target_id is not None:\n",
        "        status_text += f' | Target ID: {target_id}'\n",
        "    \n",
        "    cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "    \n",
        "    # 6. DISPLAY IMAGES\n",
        "    scale = 0.3  # Scale down for network efficiency\n",
        "    resized_color = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
        "    resized_depth = cv2.resize(depth_colormap, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
        "    display_color.value = bgr8_to_jpeg(resized_color)\n",
        "    display_depth.value = bgr8_to_jpeg(resized_depth)\n",
        "\n",
        "\n",
        "# Attach callback to camera updates\n",
        "camera.observe(main_callback, names=['color_value'])\n",
        "\n",
        "print('\\n' + '='*50)\n",
        "print('ðŸ¤– PERSON FOLLOWING SYSTEM READY')\n",
        "print('='*50)\n",
        "print('\\nKeyboard Controls:')\n",
        "print('  g = GO (start following first detected person)')\n",
        "print('  s = STOP (halt robot and reset target)')\n",
        "print('='*50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
