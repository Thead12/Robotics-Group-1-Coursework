{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c616845c3d4fbdafc5ff3683c2e813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#widgets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "import motors\n",
    "\n",
    "# Create widgets for displaying images\n",
    "display_color = widgets.Image(format='jpeg', width='45%')\n",
    "display_depth = widgets.Image(format='jpeg', width='45%')\n",
    "layout = widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth], layout=layout)\n",
    "display(sidebyside)\n",
    "\n",
    "# Convert numpy array to jpeg coded data for displaying\n",
    "def bgr8_to_jpeg(value):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check for and set the actual parameters for the camera device.\n",
    "cam_hfov = 101\n",
    "cam_vfov = 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"yolo11l_half.engine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-14 11:47:59 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-11-14 11:47:59 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-11-14 11:47:59 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-11-14 11:48:00 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-11-14 11:48:01 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-11-14 11:48:01 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-11-14 11:48:01 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-11-14 11:48:01 UTC][ZED][INFO] [Init]  Serial Number: S/N 33971626\n",
      "Loading yolo11l_half.engine for TensorRT inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5 (_capture_frames):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_5158/1568382331.py\", line 57, in _capture_frames\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/traitlets.py\", line 716, in __set__\n",
      "    self.set(obj, value)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/traitlets.py\", line 706, in set\n",
      "    obj._notify_trait(self.name, old_value, new_value)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/traitlets.py\", line 1513, in _notify_trait\n",
      "    self.notify_change(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/traitlets.py\", line 1525, in notify_change\n",
      "    return self._notify_observers(change)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/traitlets.py\", line 1568, in _notify_observers\n",
      "    c(event)\n",
      "  File \"/tmp/ipykernel_5158/4041493537.py\", line 54, in main_callback\n",
      "TypeError: SpringFollower.drive() missing 1 required positional argument: 'angle'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'float'> Value 35.470238095238095\n",
      "Turning_speed =  -0.1223193794670531\n",
      "Turning:  -0.3057984486676327\n",
      "turning right\n"
     ]
    }
   ],
   "source": [
    "import traitlets\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyzed.sl as sl\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import threading\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "# Define a Camera class that inherits from SingletonConfigurable\n",
    "class Camera(SingletonConfigurable):\n",
    "    color_value = traitlets.Any() # monitor the color_value variable\n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "\n",
    "        self.zed = sl.Camera()\n",
    "        # Create a InitParameters object and set configuration parameters\n",
    "        init_params = sl.InitParameters()\n",
    "        init_params.camera_resolution = sl.RESOLUTION.VGA #VGA(672*376), HD720(1280*720), HD1080 (1920*1080) or ...\n",
    "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA  # Use ULTRA depth mode\n",
    "        init_params.coordinate_units = sl.UNIT.MILLIMETER  # Use meter units (for depth measurements)\n",
    "\n",
    "        # Open the camera\n",
    "        status = self.zed.open(init_params)\n",
    "        if status != sl.ERROR_CODE.SUCCESS: #Ensure the camera has opened succesfully\n",
    "            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "\n",
    "         # Create and set RuntimeParameters after opening the camera\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "\n",
    "        #flag to control the thread\n",
    "        self.thread_runnning_flag = False\n",
    "\n",
    "        # Get the height and width\n",
    "        camera_info = self.zed.get_camera_information()\n",
    "        self.width = camera_info.camera_configuration.resolution.width\n",
    "        self.height = camera_info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(self.width,self.height,sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "        self.point_cloud = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C4, sl.MEM.CPU) \n",
    "\n",
    "    def _capture_frames(self): #For data capturing only\n",
    "\n",
    "        while(self.thread_runnning_flag==True): #continue until the thread_runnning_flag is set to be False\n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "                \n",
    "                # Retrieve Left image\n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "                # Retrieve depth map. Depth is aligned on the left image\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "    \n",
    "                self.color_value = self.image.get_data()\n",
    "                self.color_value= cv2.cvtColor(self.color_value, cv2.COLOR_BGRA2BGR)\n",
    "                self.depth_image = np.asanyarray(self.depth.get_data())\n",
    "\n",
    "    def pixel_to_angle(self, x, y, hfov=cam_hfov, vfov=cam_vfov):\n",
    "        \"\"\"\n",
    "        Calculate horizontal, vertical, and Euclidean viewing angles for a given pixel.\n",
    "\n",
    "        Parameters:\n",
    "            x (float): Pixel x-coordinate\n",
    "            y (float): Pixel y-coordinate\n",
    "            width (int): Image width in pixels\n",
    "            height (int): Image height in pixels\n",
    "            hfov (float): Horizontal field of view in degrees\n",
    "            vfov (float): Vertical field of view in degrees\n",
    "    \n",
    "        Returns:\n",
    "            (h_angle, v_angle, euclidean_angle): Tuple of three angles in degrees\n",
    "        \"\"\"\n",
    "        cx, cy = self.width / 2, self.height / 2\n",
    "    \n",
    "        h_angle = ((x - cx) / cx) * (hfov / 2)\n",
    "        v_angle = ((y - cy) / cy) * (vfov / 2)\n",
    "    \n",
    "        euclidean_angle = math.sqrt(h_angle**2 + v_angle**2)\n",
    "    \n",
    "        if x < cx:\n",
    "            euclidean_angle = -euclidean_angle\n",
    "    \n",
    "        return h_angle #, v_angle, euclidean_angle\n",
    "                \n",
    "    def start(self): #start the data capture thread\n",
    "        if self.thread_runnning_flag == False: #only process if no thread is running yet\n",
    "            self.thread_runnning_flag=True #flag to control the operation of the _capture_frames function\n",
    "            self.thread = threading.Thread(target=self._capture_frames) #link thread with the function\n",
    "            self.thread.start() #start the thread\n",
    "\n",
    "    def stop(self): #stop the data capture thread\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False #exit the while loop in the _capture_frames\n",
    "            self.thread.join() #wait the exiting of the thread       \n",
    "\n",
    "def bgr8_to_jpeg(value):#convert numpy array to jpeg coded data for displaying \n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera()\n",
    "camera.start() # start capturing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_human(frame):\n",
    "    \"\"\"Common function to detect humans and return closest human info\"\"\"\n",
    "    results = model(frame, verbose=False)\n",
    "    \n",
    "    conf_threshold = 0.5\n",
    "    human_detected = False\n",
    "    min_distance = float('inf')\n",
    "    closest_bbox = None\n",
    "    \n",
    "    for result in results:\n",
    "        for i in range(len(result.boxes.cls)):\n",
    "            if result.boxes.cls[i] == 0:  # Human subject\n",
    "                if result.boxes.conf[i] > conf_threshold:\n",
    "                    human_detected = True\n",
    "                    bbox = result.boxes.xyxy[i]\n",
    "                    \n",
    "                    # Calculate center of bounding box\n",
    "                    center_x = int((bbox[0] + bbox[2]) / 2)\n",
    "                    center_y = int((bbox[1] + bbox[3]) / 2)\n",
    "                    \n",
    "                    # Get depth at human center\n",
    "                    if (0 <= center_y < camera.height and 0 <= center_x < camera.width):\n",
    "                        distance = camera.depth_image[center_y, center_x]\n",
    "                        \n",
    "                        if not np.isnan(distance) and distance > 0:\n",
    "                            if distance < min_distance:\n",
    "                                min_distance = distance\n",
    "                                closest_bbox = bbox\n",
    "                                \n",
    "    \n",
    "    return human_detected, min_distance, closest_bbox, [center_x, center_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpringFollower:\n",
    "    def __init__(self, speed=0.5, k=10.0, b=6.0, rest_length=0.5, dt=0.02):\n",
    "        self.speed = speed\n",
    "        self.motors = motors.MotorsYukon(mecanum=False)\n",
    "\n",
    "        self.was_turning_right = True\n",
    "                                         \n",
    "        self.k = k\n",
    "        self.b = b\n",
    "        self.l0 = rest_length\n",
    "        self.dt = dt\n",
    "\n",
    "        self.vel = np.array([0.0, 0.0])\n",
    "        self._origin = np.array([0.0, 0.0])\n",
    "\n",
    "    def human_position(self, r, theta):\n",
    "        x = r * np.cos(theta)\n",
    "        y = r * np.sin(theta)\n",
    "        return np.array([x, y])\n",
    "\n",
    "    def update(self, human_position):\n",
    "\n",
    "        distance = human_position[0]\n",
    "        angle = human_position[1]\n",
    "        \n",
    "        ph = self.human_position(human_position[0], human_position[1])\n",
    "        diff = ph - self._origin\n",
    "        dist = np.linalg.norm(diff)\n",
    "\n",
    "        if dist > 1e-6:\n",
    "            vel_spring = (self.k / self.b) * (1 - self.l0 / dist) * diff\n",
    "        else:\n",
    "            vel_spring = np.zeros(2)\n",
    "\n",
    "        self.vel = vel_spring / self.b\n",
    "        return self.vel\n",
    "\n",
    "    def drive(self, human_detected, angle):\n",
    "        # max angle is 90 degrees\n",
    "        # at -90 and 90 degrees wheel multiplier should be 0 for corresponding side\n",
    "        # as robot is turning theta decreeses\n",
    "        print(\"Type: \", type(angle), \"Value\", angle)\n",
    "        angle_clipped = angle\n",
    "        np.clip(angle_clipped, 1, 1)\n",
    "\n",
    "        if not human_detected:\n",
    "            if self.was_turning_right:\n",
    "                angle_clipped = math.radians(90)\n",
    "            else:\n",
    "                angle_clipped = math.radians(-90)\n",
    "                \n",
    "        turning = 0.5*np.cos(angle)\n",
    "\n",
    "        turning_speed = turning*self.speed\n",
    "        print(\"Turning_speed = \", turning_speed)\n",
    "        \n",
    "        print(\"Turning: \", turning)\n",
    "        if angle_clipped > 0:\n",
    "            print(\"turning right\")\n",
    "            self.motors.frontLeft(speed=self.speed)\n",
    "            self.motors.backLeft(speed=self.speed)\n",
    "\n",
    "            self.motors.frontRight(speed=turning_speed)\n",
    "            self.motors.backRight(speed=turning_speed)\n",
    "\n",
    "            return True\n",
    "\n",
    "        elif angle_clipped < 0:            \n",
    "            print(\"turning left\")\n",
    "\n",
    "            self.motors.frontLeft(speed=turning_speed)\n",
    "            self.motors.backLeft(speed=turning_speed)\n",
    "\n",
    "            self.motors.frontRight(speed=self.speed)\n",
    "            self.motors.backRight(speed=self.speed)\n",
    "\n",
    "        else:\n",
    "            self.motors.frontLeft(speed=self.speed)\n",
    "            self.motors.backLeft(speed=self.speed)\n",
    "\n",
    "            self.motors.frontRight(speed=self.speed)\n",
    "            self.motors.backRight(speed=self.speed)\n",
    "\n",
    "            return False\n",
    "    \n",
    "    def move(self, angle):\n",
    "        self.drive(angle)    \n",
    "    \n",
    "    \n",
    "    def stop(self):\n",
    "        # turn of cameras and such\n",
    "        self.motors.stop()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create follower\n",
    "follower = SpringFollower(speed=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355e3f86352b4cccaf1d2d5ef9fa8f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Behavior:', placeholder='Type: l=Love, f=Fear, a=Aggressive, c=Curious, s=Stop')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Braitenberg Behaviors Active ===\n",
      "Press keys to switch behaviors:\n",
      "  l = Love (follow human)\n",
      "  f = Fear (retreat from human)\n",
      "  a = Aggressive (charge at human)\n",
      "  c = Curious (investigate human)\n",
      "  s = Stop (halt robot)\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# Create text input widget for keyboard control\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type: l=Love, f=Fear, a=Aggressive, c=Curious, s=Stop',\n",
    "    description='Behavior:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "def on_text_change(change):\n",
    "    \"\"\"Handle keyboard input to switch behaviors\"\"\"\n",
    "    global current_behavior\n",
    "    input_value = change['new']\n",
    "    \n",
    "    if len(input_value) > 0:\n",
    "        last_char = input_value[-1].lower()\n",
    "        \n",
    "        if last_char == 'l':\n",
    "            current_behavior = 'love'\n",
    "            print('\\nSwitched to LOVE behavior')\n",
    "        elif last_char == 'f':\n",
    "            current_behavior = 'fear'\n",
    "            print('\\nSwitched to FEAR behavior')\n",
    "        elif last_char == 'a':\n",
    "            current_behavior = 'aggressive'\n",
    "            print('\\nSwitched to AGGRESSIVE behavior')\n",
    "        elif last_char == 'c':\n",
    "            current_behavior = 'curious'\n",
    "            print('\\nSwitched to CURIOUS behavior')\n",
    "        elif last_char == 's':\n",
    "            current_behavior = 'stop'\n",
    "            robot.stop()\n",
    "            print('\\nSTOPPED - Robot halted')\n",
    "\n",
    "text_input.observe(on_text_change, names='value')\n",
    "display(text_input)\n",
    "\n",
    "def main_callback(change):\n",
    "    \"\"\"Main callback that executes current behavior\"\"\"    \n",
    "    frame = change['new']\n",
    "    \n",
    "    # Prepare depth colormap for display\n",
    "    depth_colormap = cv2.applyColorMap(\n",
    "        cv2.convertScaleAbs(camera.depth_image, alpha=0.03), \n",
    "        cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Detect human\n",
    "    human_detected, distance, bbox, center = detect_human(frame)\n",
    "    \n",
    "    angle = camera.pixel_to_angle(center[0], center[1])\n",
    "\n",
    "    follower.drive(human_detected, angle)    \n",
    "    \n",
    "    # Display images\n",
    "    scale = 0.1\n",
    "    resized_color = cv2.resize(frame, None, fx=scale, fy=scale, \n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "    resized_depth = cv2.resize(depth_colormap, None, fx=scale, fy=scale, \n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "    display_color.value = bgr8_to_jpeg(resized_color)\n",
    "    display_depth.value = bgr8_to_jpeg(resized_depth)\n",
    "\n",
    "# Attach callback to camera\n",
    "camera.observe(main_callback, names=['color_value'])\n",
    "\n",
    "print('\\n=== Braitenberg Behaviors Active ===')\n",
    "print('Press keys to switch behaviors:')\n",
    "print('  l = Love (follow human)')\n",
    "print('  f = Fear (retreat from human)')\n",
    "print('  a = Aggressive (charge at human)')\n",
    "print('  c = Curious (investigate human)')\n",
    "print('  s = Stop (halt robot)')\n",
    "print('===================================')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#follower.stop()\n",
    "#camera.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
