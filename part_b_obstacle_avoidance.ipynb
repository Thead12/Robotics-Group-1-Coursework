{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A\n",
    "\n",
    "- **Love**: Follow and maintain optimal distance\n",
    "- **Fear**: Retreat from detected humans\n",
    "- **Aggressive**: Relentless pursuit\n",
    "- **Curious**: Cautious investigation with circling\n",
    "\n",
    "Keyboard controls:\n",
    "- 'l' = Love behavior\n",
    "- 'f' = Fear behavior  \n",
    "- 'a' = Aggressive behavior\n",
    "- 'c' = Curious behavior\n",
    "- 's' = Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Display Widgets**\n",
    "\n",
    "Standard widget setup for displaying color and depth images side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c9441c4c8b4375bfc119cb3b9b962f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "\n",
    "# Create two widgets for displaying images\n",
    "display_color = widgets.Image(format='jpeg', width='45%')\n",
    "display_depth = widgets.Image(format='jpeg', width='45%')\n",
    "layout = widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth], layout=layout)\n",
    "display(sidebyside)\n",
    "\n",
    "# convert numpy array to jpeg for displaying\n",
    "def bgr8_to_jpeg(value):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Load YOLO Model**\n",
    "\n",
    "load the TensorRT YOLO model for human detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo11l_half.engine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class HistogramPersonIdentifier:\n",
    "    \"\"\"\n",
    "    Simple HSV-histogram-based person re-identification.\n",
    "    Keeps an in-memory DB of person_id -> histogram.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, similarity_thresh=0.6, h_bins=16, s_bins=16):\n",
    "        self.person_db = {}   # person_id -> hist\n",
    "        self.next_id = 0\n",
    "        self.similarity_thresh = similarity_thresh\n",
    "        self.h_bins = h_bins\n",
    "        self.s_bins = s_bins\n",
    "\n",
    "    def _get_histogram(self, roi_bgr):\n",
    "        \"\"\"Compute normalised HSV histogram for a ROI.\"\"\"\n",
    "        hsv = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2HSV)\n",
    "        hist = cv2.calcHist(\n",
    "            [hsv], [0, 1], None,\n",
    "            [self.h_bins, self.s_bins],\n",
    "            [0, 180, 0, 256]\n",
    "        )\n",
    "        cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)\n",
    "        return hist\n",
    "\n",
    "    def _match_person(self, hist):\n",
    "        \"\"\"Return best-matching person ID or None.\"\"\"\n",
    "        best_id = None\n",
    "        best_score = -1\n",
    "\n",
    "        for pid, ref_hist in self.person_db.items():\n",
    "            score = cv2.compareHist(ref_hist, hist, cv2.HISTCMP_CORREL)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_id = pid\n",
    "\n",
    "        if best_score > self.similarity_thresh:\n",
    "            return best_id\n",
    "        return None\n",
    "\n",
    "    def assign_ids(self, frame, detections):\n",
    "        \"\"\"\n",
    "        For each person box, assign a stable ID based on histogram.\n",
    "        Returns list of (x1, y1, x2, y2, person_id).\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        for box in detections.boxes:\n",
    "            cls = int(box.cls[0])\n",
    "            if cls != 0:\n",
    "                continue  # only PERSON class\n",
    "\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "            if roi.size == 0:\n",
    "                continue\n",
    "\n",
    "            hist = self._get_histogram(roi)\n",
    "            pid = self._match_person(hist)\n",
    "\n",
    "            # New identity if no good match\n",
    "            if pid is None:\n",
    "                pid = self.next_id\n",
    "                self.person_db[pid] = hist\n",
    "                self.next_id += 1\n",
    "\n",
    "            results.append((x1, y1, x2, y2, pid))\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Camera System Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-21 12:00:01 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-11-21 12:00:01 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-11-21 12:00:01 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-11-21 12:00:02 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-11-21 12:00:03 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-11-21 12:00:03 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-11-21 12:00:03 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-11-21 12:00:03 UTC][ZED][INFO] [Init]  Serial Number: S/N 35129214\n",
      "Loading yolo11l_half.engine for TensorRT inference...\n"
     ]
    }
   ],
   "source": [
    "# ZED camera initialization\n",
    "import traitlets\n",
    "import numpy as np\n",
    "import pyzed.sl as sl\n",
    "import threading\n",
    "import motors\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "# initialize robot motor control\n",
    "robot = motors.MotorsYukon(mecanum=False)\n",
    "\n",
    "# Camera class with traitlets\n",
    "class Camera(SingletonConfigurable):\n",
    "    color_value = traitlets.Any()\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "\n",
    "        # ZED camera initialization\n",
    "        self.zed = sl.Camera()\n",
    "        init_params = sl.InitParameters()\n",
    "        init_params.camera_resolution = sl.RESOLUTION.VGA  # 672x376\n",
    "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "        init_params.coordinate_units = sl.UNIT.MILLIMETER  # Distance in mm\n",
    "\n",
    "        # Open camera\n",
    "        status = self.zed.open(init_params)\n",
    "        if status != sl.ERROR_CODE.SUCCESS:\n",
    "            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "        self.thread_runnning_flag = False\n",
    "\n",
    "        # Get camera resolution\n",
    "        camera_info = self.zed.get_camera_information()\n",
    "        self.width = camera_info.camera_configuration.resolution.width\n",
    "        self.height = camera_info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(self.width, self.height, sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(self.width, self.height, sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "        self.point_cloud = sl.Mat(self.width, self.height, sl.MAT_TYPE.F32_C4, sl.MEM.CPU)\n",
    "\n",
    "    # threaded frame capture\n",
    "    def _capture_frames(self):\n",
    "        while(self.thread_runnning_flag == True):\n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "                # Retrieve images\n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "                \n",
    "                # Convert BGRA to BGR \n",
    "                self.color_value_BGRA = self.image.get_data()\n",
    "                self.color_value = cv2.cvtColor(self.color_value_BGRA, cv2.COLOR_BGRA2BGR)\n",
    "                # Get depth as numpy array\n",
    "                self.depth_image = np.asanyarray(self.depth.get_data())\n",
    "                \n",
    "    # thread control methods\n",
    "    def start(self):\n",
    "        if self.thread_runnning_flag == False:\n",
    "            self.thread_runnning_flag = True\n",
    "            self.thread = threading.Thread(target=self._capture_frames)\n",
    "            self.thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False\n",
    "            self.thread.join()\n",
    "            robot.stop()\n",
    "\n",
    "# Initialize and start camera\n",
    "camera = Camera()\n",
    "camera.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Behavior Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable for current behavior mode\n",
    "current_behavior = 'stop'\n",
    "\n",
    "identifier = HistogramPersonIdentifier(similarity_thresh=0.6)\n",
    "\n",
    "def detect_human(frame):\n",
    "    global identifier\n",
    "    \n",
    "    \"\"\"\n",
    "    Human detection and distance calculation\n",
    "    \n",
    "    From Tutorial 5:\n",
    "    - YOLO detection for humans (class 0)\n",
    "    - Bounding box extraction: result.boxes.xyxy[i]\n",
    "    - Center calculation: (bbox[0] + bbox[2]) / 2 for x, (bbox[1] + bbox[3]) / 2 for y\n",
    "    - Distance extraction: camera.depth_image[center_y, center_x]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    results = model(frame, verbose=False)[0]\n",
    "    \n",
    "    # confidence threshold for detections\n",
    "    conf_threshold = 0.5\n",
    "    human_detected = False\n",
    "    min_human_human_human_human_human_human_distance = float('inf')\n",
    "    closest_bbox = None\n",
    "\n",
    "    tracks = identifier.assign_ids(frame, results)\n",
    "    if len(tracks) == 0:\n",
    "        return human_detected, min_distance, closest_bbox\n",
    "    \n",
    "    x1, y1, x2, y2, pid = tracks[0]\n",
    "    center_x = int((x1 + x2) / 2)\n",
    "    center_y = int((y1 + y2) / 2)\n",
    "    distance = camera.depth_image[center_y, center_x]\n",
    "    closest_bbox = [x1, y1, x2, y2]                \n",
    "    \n",
    "    return human_detected, min_distance, closest_bbox\n",
    "\n",
    "def detect_obstacle(depth_image, min_distance):\n",
    "    depth = np.nan_to_num(depth_image.copy(), nan=0).astype(np.float32)\n",
    "\n",
    "    # Crop region of interest\n",
    "    depth[:200, :] = 0\n",
    "    depth[376:, :] = 0\n",
    "    depth[:, :168] = 0\n",
    "    depth[:, 504:] = 0\n",
    "\n",
    "    # Filter out noise\n",
    "    depth[(depth < 100) | (depth > 1000)] = 0\n",
    "\n",
    "    # If nothing valid is detected\n",
    "    if depth.max() == 0:\n",
    "        return \"no_depth\", None\n",
    "\n",
    "    # Find closest obstacle\n",
    "    distance = depth[depth != 0].min()\n",
    "\n",
    "    if distance < min_distance:\n",
    "        return True, distance\n",
    "    else:\n",
    "        return False, distance\n",
    "\n",
    "\n",
    "def love_behavior(frame, depth_colormap, human_detected, human_distance, bbox):\n",
    "    \"\"\"\n",
    "    Love behavior - follow human and maintain comfortable distance\n",
    "    \n",
    "    Distance thresholds chosen based on testing:\n",
    "    - < 450mm: Too close, back up\n",
    "    - 450-600mm: Optimal distance, stop\n",
    "    - 600-1200mm: Medium range, approach slowly\n",
    "    - > 1200mm: Far away, approach faster\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if human_detected and human_distance != float('inf'):\n",
    "        # Draw detection box (Tutorial 5 pattern)\n",
    "        cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), \n",
    "                     (int(bbox[2]), int(bbox[3])), (255, 105, 180), 2)\n",
    "        \n",
    "        # Calculate if human is left/right of center (Tutorial 5 approach)\n",
    "        bbox_center_x = (bbox[0] + bbox[2]) / 2\n",
    "        frame_center_x = camera.width / 2\n",
    "        \n",
    "        # Distance-based behavior (new thresholds for love behavior)\n",
    "        if human_distance < 450:\n",
    "            robot.backward(0.2)\n",
    "            cv2.putText(depth_colormap, f'LOVE: Too close {human_distance:.0f}mm', \n",
    "                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "        elif human_distance >= 450 and human_distance <= 600:\n",
    "            robot.stop()\n",
    "            cv2.putText(depth_colormap, f'LOVE: Perfect {human_distance:.0f}mm <3', \n",
    "                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)\n",
    "        elif human_distance > 600 and human_distance <= 1200:\n",
    "            # Medium distance - turn to center, then approach\n",
    "            # 50 pixel threshold to avoid constant adjustment\n",
    "            if bbox_center_x < frame_center_x - 50:\n",
    "                robot.left(0.3) \n",
    "            elif bbox_center_x > frame_center_x + 50:\n",
    "                robot.right(0.3) \n",
    "            else:\n",
    "                robot.forward(0.3) \n",
    "            cv2.putText(depth_colormap, f'LOVE: Approaching {human_distance:.0f}mm', \n",
    "                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 180, 0), 2)\n",
    "        else:  # distance > 1200\n",
    "            # Far away - approach faster with turning\n",
    "            if bbox_center_x < frame_center_x - 50:\n",
    "                robot.left(0.4)\n",
    "            elif bbox_center_x > frame_center_x + 50:\n",
    "                robot.right(0.4)\n",
    "            else:\n",
    "                robot.forward(0.5)\n",
    "            cv2.putText(depth_colormap, f'LOVE: Coming {human_distance:.0f}mm', \n",
    "                       (150, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 0), 2)\n",
    "    else:\n",
    "        # No human detected - stop and wait\n",
    "        robot.stop()\n",
    "        cv2.putText(depth_colormap, 'LOVE: Waiting...', \n",
    "                   (200, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 2)\n",
    "        \n",
    "def avoid_obstacle(human_detected, obstacle_detected, obstacle_distance):\n",
    "    global avoid_state  # internal state for the avoidance routine\n",
    "\n",
    "    if avoid_state == \"start\":\n",
    "        robot.turn_left(0.4)\n",
    "        if not obstacle_detected:\n",
    "            avoid_state = \"following_edge\"\n",
    "        return False  # still avoiding\n",
    "\n",
    "    elif avoid_state == \"following_edge\":\n",
    "        if not obstacle_detected:\n",
    "            robot.right(0.3)\n",
    "\n",
    "        elif obstacle_detected:\n",
    "            robot.forward(0.3)\n",
    "            if obstacle_distance < 200:\n",
    "                robot.turn_left(0.3)\n",
    "        elif human_detected and not obstacle_detected:\n",
    "            avoid_state = \"finished\"\n",
    "        return False\n",
    "\n",
    "    elif avoid_state == \"finished\":\n",
    "        avoid_state = \"start\"   # reset for next obstacle\n",
    "        return True              # avoidance is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Keyboard Control and Main Callback**\n",
    "\n",
    "Based on Answer_keyboard_control.ipynb pattern for keyboard input handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bd115c299a445eb4237505d8898b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Behavior:', placeholder='Type: g=Go, s=Stop')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press keys to switch modes:\n",
      "  g = Go (start robot)\n",
      "  s = Stop (halt robot)\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type: g=Go, s=Stop',\n",
    "    description='Behavior:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# observe text changes\n",
    "def on_text_change(change):\n",
    "    \"\"\"\n",
    "    Handle keyboard input to switch behaviors\n",
    "    \"\"\"\n",
    "    global current_behavior\n",
    "    input_value = change['new']\n",
    "    \n",
    "    if len(input_value) > 0:\n",
    "        last_char = input_value[-1].lower()\n",
    "\n",
    "        # Switch behavior based on input\n",
    "        if last_char == 'g':\n",
    "            current_behavior = 'go'\n",
    "            print('\\nSTARTED - Robot started')\n",
    "        elif last_char == 's':\n",
    "            current_behavior = 'stop'\n",
    "            robot.stop()\n",
    "            print('\\nSTOPPED - Robot halted')\n",
    "\n",
    "text_input.observe(on_text_change, names='value')\n",
    "display(text_input)\n",
    "\n",
    "def main_callback(change):\n",
    "    \"\"\"\n",
    "    Main callback function - executes current behavior    \n",
    "    \"\"\"\n",
    "    # Get updated frame\n",
    "    frame = change['new']\n",
    "    \n",
    "    # Create depth colormap for display\n",
    "    # Converts depth values to color visualization\n",
    "    depth_image = camera.depth_image\n",
    "    depth_colormap = cv2.applyColorMap(\n",
    "        cv2.convertScaleAbs(depth_image, alpha=0.03), \n",
    "        cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Detect humans and measure distance (Tutorial 5 method)\n",
    "    human_detected, human_distance, bbox = detect_human(frame)\n",
    "    obstacle_detected, obstacle_distance = detect_obstacle(depth_image, 400)\n",
    "    \n",
    "    # Execute behavior based on current mode\n",
    "    if current_behavior == 'go':\n",
    "        if obstacle_detected:\n",
    "            current_behavior = 'avoid'\n",
    "\n",
    "        love_behavior(frame, depth_colormap, human_detected, human_distance, bbox)\n",
    "    elif current_behavior == 'avoid':\n",
    "\n",
    "        done = avoid_obstacle(obstacle_distance)  \n",
    "\n",
    "        if done:\n",
    "            current_behavior = 'go'\n",
    "            \n",
    "    elif current_behavior == 'stop':\n",
    "        robot.stop()\n",
    "        cv2.putText(depth_colormap, 'STOPPED', \n",
    "                   (250, 188), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 2)\n",
    "    \n",
    "    # Display current behavior mode on frame\n",
    "    cv2.putText(frame, f'Mode: {current_behavior.upper()}', \n",
    "               (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    \n",
    "    # Display images\n",
    "    scale = 0.3\n",
    "    resized_color = cv2.resize(frame, None, fx=scale, fy=scale, \n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "    resized_depth = cv2.resize(depth_colormap, None, fx=scale, fy=scale, \n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "    display_color.value = bgr8_to_jpeg(resized_color)\n",
    "    display_depth.value = bgr8_to_jpeg(resized_depth)\n",
    "\n",
    "camera.observe(main_callback, names=['color_value'])\n",
    "\n",
    "print('Press keys to switch modes:')\n",
    "print('  g = Go (start robot)')\n",
    "print('  s = Stop (halt robot)')\n",
    "print('===================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To stop the system:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System stopped - camera and robot halted\n"
     ]
    }
   ],
   "source": [
    "# stop camera thread and motors\n",
    "#camera.stop()\n",
    "print('System stopped - camera and robot halted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Notes\n",
    "\n",
    "### Distance Calculation\n",
    "Distance is measured by:\n",
    "1. Getting bounding box from YOLO: `bbox = result.boxes.xyxy[i]` (format: [x1, y1, x2, y2])\n",
    "2. Calculating center: `center_x = (bbox[0] + bbox[2]) / 2`, `center_y = (bbox[1] + bbox[3]) / 2`\n",
    "3. Reading depth at center: `distance = camera.depth_image[center_y, center_x]`\n",
    "4. Depth image returns distance in millimeters\n",
    "\n",
    "\n",
    "### Behavior Distance Thresholds:\n",
    "\n",
    "**Love:** 450mm (too close) | 450-600mm (perfect) | 600-1200mm (medium) | >1200mm (far)\n",
    "\n",
    "**Fear:** 500mm (danger) | 900mm (caution) | 1300mm (alert) | >1300mm (safe)\n",
    "\n",
    "**Aggressive:** 300mm (ram) | 600mm (engage) | 1000mm (rush) | >1000mm (chase)\n",
    "\n",
    "**Curious:** 400mm (investigate) | 800mm (approach) | >800mm (interested)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
